{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import gluonnlp as nlp\n",
    "import pkuseg\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from d2l import try_gpu\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import d2l\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "WORD_EMBED = 'sgns.weibo.bigram-char'\n",
    "LABEL_FILE = 'train.label'\n",
    "N_ROWS=1000\n",
    "ctx = try_gpu()\n",
    "seg = pkuseg.pkuseg(model_name='web')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_FOLDER+TRAIN_DATA, sep='|', nrows=N_ROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =[ [row[0], row[1]] for _, row in train_df.iterrows()]\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, .01)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=0.29s, #Sentences=990\n",
      "Done! Tokenizing Time=0.30s, #Sentences=10\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(x):\n",
    "    tweet, label = x\n",
    "    if type(tweet) != str:\n",
    "        tweet = str(tweet)\n",
    "    word_list = seg.cut(tweet)\n",
    "    return word_list, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def to_word_list(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(tokenizer, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_word_list, train_word_lengths = to_word_list(train_dataset)\n",
    "valid_word_list, valid_word_lengths = to_word_list(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/embedding/token_embedding.py:296: UserWarning: line 0 in data/sgns.weibo.bigram-char: skipped likely header line.\n",
      "  .format(line_num, pretrained_file_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=4100, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "train_seqs = [sample[0] for sample in train_word_list]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=100000)\n",
    "\n",
    "# load customed pre-trained embedding\n",
    "embedding_weights = nlp.embedding.TokenEmbedding.from_file(file_path=DATA_FOLDER+WORD_EMBED)\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_word_list)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=990, batch_num=10\n",
      "  key=[12, 21, 30, 39, 48, 57, 66, 75, 84, 93]\n",
      "  cnt=[650, 167, 64, 42, 15, 16, 20, 5, 8, 3]\n",
      "  batch_size=[992, 566, 396, 305, 256, 256, 256, 256, 256, 256]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # in this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_word_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[3.067e+03 6.490e+02 2.322e+03 ... 4.007e+03 8.000e+00 0.000e+00]\n",
      " [3.500e+01 2.510e+02 2.000e+01 ... 2.400e+01 8.000e+00 0.000e+00]\n",
      " [1.339e+03 1.950e+02 1.051e+03 ... 9.900e+01 9.800e+01 0.000e+00]\n",
      " ...\n",
      " [3.400e+01 1.030e+02 4.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.544e+03 3.815e+03 5.000e+02 ... 9.090e+02 1.300e+01 0.000e+00]\n",
      " [5.200e+01 1.829e+03 5.800e+01 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "<NDArray 42x39 @cpu_shared(0)> \n",
      "[40 31 38  8 35 20 30 15 45 27 24  3 53 56 23 51 29 16 36  3  3 59 13 34\n",
      " 18 32  6 22 14 19  3 22 35 54 24 30 33  2 30 34  2  3]\n",
      "<NDArray 42 @cpu_shared(0)>\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in train_dataloader:\n",
    "    print(tweet, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model contruction\n",
    "Self attention layer, weighted cross entropy, and whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Block):\n",
    "    def __init__(self, vocab_len, embed_size, kernel_sizes, num_channels, \\\n",
    "                 dropout, nclass, **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_len, embed_size)\n",
    "        \n",
    "        self.constant_embedding = nn.Embedding(vocab_len, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.decoder = nn.Dense(nclass)\n",
    "        self.pool = nn.GlobalMaxPool1D()\n",
    "        self.convs = nn.Sequential()  \n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.add(nn.Conv1D(c, k, activation='relu'))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeddings = nd.concat(\n",
    "            self.embedding(inputs), self.constant_embedding(inputs), dim=2)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.transpose((0, 2, 1))\n",
    "        \n",
    "        encoding = nd.concat(*[nd.flatten(\n",
    "            self.pool(conv(embeddings))) for conv in self.convs], dim=1)\n",
    "        print(encoding.shape)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embedding): Embedding(4100 -> 300, float32)\n",
      "  (constant_embedding): Embedding(4100 -> 300, float32)\n",
      "  (dropout): Dropout(p = 0.4, axes=())\n",
      "  (decoder): Dense(None -> 72, linear)\n",
      "  (pool): GlobalMaxPool1D(size=(1,), stride=(1,), padding=(0,), ceil_mode=True)\n",
      "  (convs): Sequential(\n",
      "    (0): Conv1D(None -> 200, kernel_size=(2,), stride=(1,), Activation(relu))\n",
      "    (1): Conv1D(None -> 200, kernel_size=(3,), stride=(1,), Activation(relu))\n",
      "    (2): Conv1D(None -> 200, kernel_size=(4,), stride=(1,), Activation(relu))\n",
      "    (3): Conv1D(None -> 200, kernel_size=(5,), stride=(1,), Activation(relu))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300   # word embedding size\n",
    "nhidden = 400    # lstm hidden_dim\n",
    "nlayers = 4     # lstm layers\n",
    "natt_unit = 400     # the hidden_units of attention layer\n",
    "natt_hops = 20    # the channels of attention\n",
    "nfc = 256  # last dense layer size\n",
    "nclass = 72 # we have 72 emoji in total\n",
    "\n",
    "drop_prob = 0.5\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "kernel_sizes, nums_channels = [2, 3, 4, 5], [200, 200, 200, 200]\n",
    "model = TextCNN(vocab_len, emsize, kernel_sizes, nums_channels, drop_prob, nclass)\n",
    "model.initialize(init.Xavier(), ctx=ctx)\n",
    "\n",
    "print(model)\n",
    "model.embedding.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "model.constant_embedding.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "model.constant_embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 600)\n",
      "(1, 800)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.798947   -1.5127401   0.5445767  -0.522098    1.3970379  -0.3428049\n",
       "  -1.0130728  -0.07241816  0.9931398   0.31141812 -1.3905609   0.04244024\n",
       "   0.94099516 -0.6591962   1.4808002   0.25050783 -0.13716994  0.61912596\n",
       "   0.06574339  0.43878537  1.0778569  -0.9727909   1.5894302  -0.01894315\n",
       "  -0.37659562 -0.87707716  0.9530885   0.08174714 -0.3447823   0.59718174\n",
       "   2.8320794   0.25457725 -1.1092602   0.4659843  -1.7563459   0.9833747\n",
       "  -0.45883682  0.13550195  0.6052164   1.3967222   0.9361794   0.15929052\n",
       "  -0.27935922  0.85601705  1.2627268   0.43748006 -0.99077004  2.2483907\n",
       "  -1.9119489   0.45857954  1.1022046   0.12607937  1.167786   -0.41126722\n",
       "   1.1529258   1.0230607  -0.63536763 -0.4965821  -0.73578906 -1.4766121\n",
       "   0.5688625   1.7369577   1.5292891  -0.8065958   0.7418031   0.76949096\n",
       "   1.734976    0.69301844  1.078259   -0.18670827 -0.992385    0.31959945]]\n",
       "<NDArray 1x72 @gpu(0)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = nd.array([10, 20, 30, 40, 50, 60], ctx=ctx).reshape(1, -1)\n",
    "model(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 3.6126, train acc 0.140, test acc 0.138, time 127.6 sec\n",
      "epoch 2, loss 3.5836, train acc 0.144, test acc 0.144, time 128.1 sec\n",
      "epoch 3, loss 3.5826, train acc 0.145, test acc 0.144, time 125.5 sec\n",
      "epoch 4, loss 3.5820, train acc 0.144, test acc 0.140, time 126.8 sec\n",
      "epoch 5, loss 3.5815, train acc 0.144, test acc 0.142, time 128.8 sec\n",
      "epoch 6, loss 3.5817, train acc 0.144, test acc 0.138, time 130.0 sec\n",
      "epoch 7, loss 3.5823, train acc 0.144, test acc 0.142, time 128.1 sec\n",
      "epoch 8, loss 3.5810, train acc 0.145, test acc 0.142, time 122.9 sec\n"
     ]
    }
   ],
   "source": [
    "lr, wd, num_epochs = .001, .01, 8\n",
    "# model.load_parameters(\"model/textcnn1560329753\")\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr, 'wd': wd})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train(train_dataloader, valid_dataloader, model, loss, trainer, ctx, num_epochs)\n",
    "token = str(round(time.time()))\n",
    "model.save_parameters(\"model/textcnn\"+token+'.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = str(round(time.time()))\n",
    "# model.save_parameters(\"model/model\"+token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_DATA = 'test.csv'\n",
    "predictions = []\n",
    "test_df = pd.read_csv(DATA_FOLDER+TEST_DATA, header=None, sep='\\t')\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for _, tweet in test_df.iterrows():\n",
    "    token = vocab[seg.cut(tweet[1])]\n",
    "    if len(token)<5:\n",
    "        token += [0.]*(5-len(token))\n",
    "    inp = nd.array(token, ctx=ctx).reshape(1,-1)\n",
    "    pred = model(inp)\n",
    "    pred = nd.argmax(pred, axis=1).asscalar()\n",
    "    predictions.append(int(pred))\n",
    "    if len(predictions)%2000==0:\n",
    "        ckpt = time.time()\n",
    "        print('current pred len %d, time %.2fs' % (len(predictions), ckpt-start))\n",
    "        start = ckpt\n",
    "submit = pd.DataFrame({'Expected': predictions})\n",
    "submit.to_csv('submission.csv', sep=',', index_label='ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
