{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import gluonnlp as nlp\n",
    "import pkuseg\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from d2l import try_gpu\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import d2l\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "WORD_EMBED = 'sgns.weibo.bigram-char'\n",
    "LABEL_FILE = 'train.label'\n",
    "N_ROWS=10000\n",
    "ctx = try_gpu()\n",
    "seg = pkuseg.pkuseg(model_name='web')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_FOLDER+TRAIN_DATA, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690531, 172633)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =[ [row[0], row[1]] for _, row in train_df.iterrows()]\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, .2)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=77.48s, #Sentences=690531\n",
      "Done! Tokenizing Time=20.25s, #Sentences=172633\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(x):\n",
    "    tweet, label = x\n",
    "    if type(tweet) != str:\n",
    "        tweet = str(tweet)\n",
    "    word_list = seg.cut(tweet)\n",
    "    return word_list, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def to_word_list(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(tokenizer, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_word_list, train_word_lengths = to_word_list(train_dataset)\n",
    "valid_word_list, valid_word_lengths = to_word_list(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/embedding/token_embedding.py:296: UserWarning: line 0 in data/sgns.weibo.bigram-char: skipped likely header line.\n",
      "  .format(line_num, pretrained_file_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=40004, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "train_seqs = [sample[0] for sample in train_word_list]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=40000)\n",
    "\n",
    "# load customed pre-trained embedding\n",
    "embedding_weights = nlp.embedding.TokenEmbedding.from_file(file_path=DATA_FOLDER+WORD_EMBED)\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_word_list)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=690531, batch_num=3568\n",
      "  key=[15, 28, 41, 54, 67, 80, 93, 106, 119, 132]\n",
      "  cnt=[489999, 112913, 41388, 20449, 12212, 7963, 4669, 906, 29, 3]\n",
      "  batch_size=[281, 150, 103, 78, 64, 64, 64, 64, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # in this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_word_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [8.0000e+01 0.0000e+00 7.4000e+01 ... 7.8000e+01 0.0000e+00 0.0000e+00]\n",
      " [3.7659e+04 3.4900e+02 6.8550e+03 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [1.3134e+04 4.0000e+00 2.4150e+03 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [1.0000e+01 4.1000e+01 8.0000e+01 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [2.5410e+03 2.3750e+03 7.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]\n",
      "<NDArray 281x15 @cpu_shared(0)> \n",
      "[31 12 35 42 35  7  3 33 62 22 39 15 12  3 54 34  3 12 71  3  1 34 18  3\n",
      " 17 29 16 29 46 35 14  0 35 52 69 40  8 47 12 50  3 14  3 50 41 34 15  3\n",
      " 60 50 35 61 24 14 27 12 17  1 34  3 29 42 60 15 42 20 29 17 49 35 34 15\n",
      "  3 12 35 31 58 45 24 54 49 34 29 62 12 36 12 35 51 34 52 34  2  6 29 12\n",
      " 44 35 62 60  3 35 29 16 42 35 45 34 27 15  3 55 42 34 12  6 34 16 24 54\n",
      " 24 62 55 29 18 29 17 29 62 14 23 53 39 57 29 46 27 35  3 54 42 14 32 67\n",
      " 25 15  5 12 69 69 58 64 32 37 42 29 34 52 12 11 17 16  9 16 34 68 31 35\n",
      " 23  3 30 23 14 24 35 61 30 19  9  3 44 12 61 12 36 16 12 27 50  2 16 32\n",
      " 23 31 14 41  0 21 29  3 17 34 37 14 12 28 40 23 29 29 49 13 40 31 56 29\n",
      " 52 54 29 58 10 10 34 34 68  3 23 62 19 34 39  5 24 14 12 27 30  2 10 30\n",
      " 37 34 43 23 52 27 18 18 29 35 34  1 12 60  6 19 14  3 15 13 15 14 62 42\n",
      " 19 29 21 42 29  3 29 12 54 11 51 10 19 31 18 36 29]\n",
      "<NDArray 281 @cpu_shared(0)>\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in train_dataloader:\n",
    "    print(tweet, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model contruction\n",
    "Self attention layer, weighted cross entropy, and whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Block):\n",
    "    def __init__(self, vocab_len, embed_size, kernel_sizes, num_channels, \\\n",
    "                 dropout, nclass, **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_len, embed_size)\n",
    "        \n",
    "        self.constant_embedding = nn.Embedding(vocab_len, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.decoder = nn.Dense(nclass)\n",
    "        self.pool = nn.GlobalMaxPool1D()\n",
    "        self.convs = nn.Sequential()  \n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.add(nn.Conv1D(c, k, activation='relu'))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeddings = nd.concat(\n",
    "            self.embedding(inputs), self.constant_embedding(inputs), dim=2)\n",
    "        \n",
    "        embeddings = embeddings.transpose((0, 2, 1))\n",
    "        \n",
    "        encoding = nd.concat(*[nd.flatten(\n",
    "            self.pool(conv(embeddings))) for conv in self.convs], dim=1)\n",
    "        \n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embedding): Embedding(40004 -> 300, float32)\n",
      "  (constant_embedding): Embedding(40004 -> 300, float32)\n",
      "  (dropout): Dropout(p = 0.5, axes=())\n",
      "  (decoder): Dense(None -> 72, linear)\n",
      "  (pool): GlobalMaxPool1D(size=(1,), stride=(1,), padding=(0,), ceil_mode=True)\n",
      "  (convs): Sequential(\n",
      "    (0): Conv1D(None -> 100, kernel_size=(2,), stride=(1,), Activation(relu))\n",
      "    (1): Conv1D(None -> 100, kernel_size=(3,), stride=(1,), Activation(relu))\n",
      "    (2): Conv1D(None -> 100, kernel_size=(4,), stride=(1,), Activation(relu))\n",
      "    (3): Conv1D(None -> 100, kernel_size=(5,), stride=(1,), Activation(relu))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300   # word embedding size\n",
    "nhidden = 400    # lstm hidden_dim\n",
    "nlayers = 4     # lstm layers\n",
    "natt_unit = 400     # the hidden_units of attention layer\n",
    "natt_hops = 20    # the channels of attention\n",
    "nfc = 256  # last dense layer size\n",
    "nclass = 72 # we have 72 emoji in total\n",
    "\n",
    "drop_prob = 0.5\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "kernel_sizes, nums_channels = [2, 3, 4, 5], [100, 100, 100, 100]\n",
    "model = TextCNN(vocab_len, emsize, kernel_sizes, nums_channels, drop_prob, nclass)\n",
    "model.initialize(init.Xavier(), ctx=ctx)\n",
    "\n",
    "print(model)\n",
    "model.embedding.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "model.constant_embedding.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "model.constant_embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 3.6108, train acc 0.142, test acc 0.163, time 147.5 sec\n",
      "epoch 2, loss 3.4957, train acc 0.160, test acc 0.169, time 86.3 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 20\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train(train_dataloader, valid_dataloader, model, loss, trainer, ctx, num_epochs)\n",
    "token = str(round(time.time()))\n",
    "model.export(\"model/model\"+token, epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATA = 'test.csv'\n",
    "predictions = []\n",
    "test_df = pd.read_csv(DATA_FOLDER+TEST_DATA, header=None, sep='\\t')\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.459 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.459 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pred len 2000, time 10.20s\n",
      "current pred len 4000, time 9.77s\n",
      "current pred len 6000, time 9.81s\n",
      "current pred len 8000, time 9.60s\n",
      "current pred len 10000, time 9.22s\n",
      "current pred len 12000, time 9.45s\n",
      "current pred len 14000, time 9.32s\n",
      "current pred len 16000, time 9.08s\n",
      "current pred len 18000, time 9.21s\n",
      "current pred len 20000, time 9.23s\n",
      "current pred len 22000, time 9.36s\n",
      "current pred len 24000, time 9.30s\n",
      "current pred len 26000, time 9.38s\n",
      "current pred len 28000, time 9.82s\n",
      "current pred len 30000, time 9.76s\n",
      "current pred len 32000, time 9.79s\n",
      "current pred len 34000, time 9.74s\n",
      "current pred len 36000, time 9.75s\n",
      "current pred len 38000, time 9.78s\n",
      "current pred len 40000, time 9.63s\n",
      "current pred len 42000, time 9.66s\n",
      "current pred len 44000, time 9.57s\n",
      "current pred len 46000, time 9.73s\n",
      "current pred len 48000, time 9.63s\n",
      "current pred len 50000, time 9.65s\n",
      "current pred len 52000, time 9.55s\n",
      "current pred len 54000, time 9.71s\n",
      "current pred len 56000, time 9.76s\n",
      "current pred len 58000, time 9.61s\n",
      "current pred len 60000, time 9.63s\n",
      "current pred len 62000, time 9.71s\n",
      "current pred len 64000, time 9.67s\n",
      "current pred len 66000, time 9.83s\n",
      "current pred len 68000, time 9.78s\n",
      "current pred len 70000, time 9.73s\n",
      "current pred len 72000, time 9.73s\n",
      "current pred len 74000, time 9.72s\n",
      "current pred len 76000, time 9.78s\n",
      "current pred len 78000, time 9.57s\n",
      "current pred len 80000, time 9.66s\n",
      "current pred len 82000, time 9.75s\n",
      "current pred len 84000, time 9.69s\n",
      "current pred len 86000, time 9.75s\n",
      "current pred len 88000, time 9.57s\n",
      "current pred len 90000, time 9.72s\n",
      "current pred len 92000, time 9.71s\n",
      "current pred len 94000, time 9.74s\n",
      "current pred len 96000, time 9.44s\n",
      "current pred len 98000, time 9.25s\n",
      "current pred len 100000, time 9.17s\n",
      "current pred len 102000, time 9.01s\n",
      "current pred len 104000, time 9.48s\n",
      "current pred len 106000, time 9.27s\n",
      "current pred len 108000, time 9.23s\n",
      "current pred len 110000, time 9.31s\n",
      "current pred len 112000, time 9.40s\n",
      "current pred len 114000, time 9.17s\n",
      "current pred len 116000, time 9.35s\n",
      "current pred len 118000, time 9.32s\n",
      "current pred len 120000, time 9.27s\n",
      "current pred len 122000, time 9.39s\n",
      "current pred len 124000, time 9.22s\n",
      "current pred len 126000, time 9.10s\n",
      "current pred len 128000, time 9.28s\n",
      "current pred len 130000, time 9.32s\n",
      "current pred len 132000, time 9.26s\n",
      "current pred len 134000, time 9.28s\n",
      "current pred len 136000, time 9.36s\n",
      "current pred len 138000, time 9.28s\n",
      "current pred len 140000, time 9.32s\n",
      "current pred len 142000, time 9.17s\n",
      "current pred len 144000, time 9.31s\n",
      "current pred len 146000, time 9.30s\n",
      "current pred len 148000, time 9.30s\n",
      "current pred len 150000, time 9.33s\n",
      "current pred len 152000, time 9.30s\n",
      "current pred len 154000, time 9.31s\n",
      "current pred len 156000, time 9.23s\n",
      "current pred len 158000, time 9.34s\n",
      "current pred len 160000, time 9.17s\n",
      "current pred len 162000, time 9.28s\n",
      "current pred len 164000, time 9.15s\n",
      "current pred len 166000, time 9.30s\n",
      "current pred len 168000, time 9.37s\n",
      "current pred len 170000, time 9.36s\n",
      "current pred len 172000, time 9.25s\n",
      "current pred len 174000, time 9.35s\n",
      "current pred len 176000, time 9.34s\n",
      "current pred len 178000, time 9.33s\n",
      "current pred len 180000, time 9.33s\n",
      "current pred len 182000, time 9.24s\n",
      "current pred len 184000, time 9.37s\n",
      "current pred len 186000, time 9.29s\n",
      "current pred len 188000, time 9.20s\n",
      "current pred len 190000, time 9.38s\n",
      "current pred len 192000, time 9.32s\n",
      "current pred len 194000, time 9.25s\n",
      "current pred len 196000, time 9.33s\n",
      "current pred len 198000, time 9.19s\n",
      "current pred len 200000, time 9.31s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for _, tweet in test_df.iterrows():\n",
    "    token = vocab[seg.cut(tweet[1])]\n",
    "    inp = nd.array(token, ctx=ctx).reshape(1,-1)\n",
    "    pred, _ = model(inp)\n",
    "    pred = nd.argmax(pred, axis=-1).asscalar()\n",
    "    predictions.append(int(pred))\n",
    "    if len(predictions)%2000==0:\n",
    "        ckpt = time.time()\n",
    "        print('current pred len %d, time %.2fs' % (len(predictions), ckpt-start))\n",
    "        start = ckpt\n",
    "submit = pd.DataFrame({'Expected': predictions})\n",
    "submit.to_csv('submission.csv', sep=',', index_label='ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
