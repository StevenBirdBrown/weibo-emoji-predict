{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import gluonnlp as nlp\n",
    "import pkuseg\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from d2l import try_gpu\n",
    "import itertools\n",
    "import jieba\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import d2l\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# fixed random number seed\n",
    "np.random.seed(2333)\n",
    "mx.random.seed(2333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "WORD_EMBED = 'sgns.weibo.bigram-char'\n",
    "LABEL_FILE = 'train.label'\n",
    "N_ROWS=1000\n",
    "ctx = mx.gpu(0)\n",
    "seg = pkuseg.pkuseg(model_name='web')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302867</th>\n",
       "      <td>就喜欢你臭吧拉几的帅</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293170</th>\n",
       "      <td>现在还能买到鎏金宝鉴么？</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758944</th>\n",
       "      <td>笑尿你：病房按钮乱按的爆笑后果病房按钮乱按的爆笑后果 06集 12 lol:-) 第一季谂</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10078</th>\n",
       "      <td>美图手机</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488768</th>\n",
       "      <td>哈～哈哈～哈哈哈～啊哈哈哈哈哈～</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  label\n",
       "302867                                     就喜欢你臭吧拉几的帅     13\n",
       "293170                                   现在还能买到鎏金宝鉴么？     39\n",
       "758944  笑尿你：病房按钮乱按的爆笑后果病房按钮乱按的爆笑后果 06集 12 lol:-) 第一季谂     34\n",
       "10078                                            美图手机      3\n",
       "488768                               哈～哈哈～哈哈哈～啊哈哈哈哈哈～     45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(DATA_FOLDER+TRAIN_DATA, sep='|')\n",
    "train_df = train_df.sample(frac=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820005, 43159)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =[ [row[0], row[1]] for _, row in train_df.iterrows()]\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.420 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.514 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.551 seconds.\n",
      "Loading model cost 1.507 seconds.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.617 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.625 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.550 seconds.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Prefix dict has been built succesfully.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.673 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.647 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.508 seconds.\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.573 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.640 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "Done! Tokenizing Time=24.44s, #Sentences=820005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.330 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.422 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.478 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.480 seconds.\n",
      "Loading model cost 1.491 seconds.\n",
      "Loading model cost 1.497 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.501 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.488 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.521 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.629 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.700 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.927 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=3.22s, #Sentences=43159\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(x):\n",
    "    tweet, label = x\n",
    "    if type(tweet) != str:\n",
    "        print(tweet)\n",
    "        tweet = str(tweet)\n",
    "    word_list = jieba.lcut(tweet)\n",
    "    if len(word_list)==0:\n",
    "        word_list=['<unk>']\n",
    "    return word_list, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def to_word_list(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(tokenizer, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_word_list, train_word_lengths = to_word_list(train_dataset)\n",
    "valid_word_list, valid_word_lengths = to_word_list(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=200004, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "train_seqs = [sample[0] for sample in train_word_list]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=200000)\n",
    "\n",
    "# load customed pre-trained embedding\n",
    "embedding_weights = nlp.embedding.TokenEmbedding.from_file(file_path=DATA_FOLDER+WORD_EMBED)\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_word_list)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=820005, batch_num=632\n",
      "  key=[15, 26, 37, 48, 59, 70, 81, 92, 103, 114, 125, 136, 147, 158, 169, 180, 191, 202, 213, 224]\n",
      "  cnt=[573412, 124268, 51415, 27570, 17000, 11311, 8113, 5220, 1308, 205, 83, 48, 15, 11, 7, 7, 6, 2, 2, 2]\n",
      "  batch_size=[1529, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "bucket_num = 20\n",
    "bucket_ratio = 0.1\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # in this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_word_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[2.8000e+01 4.7000e+01 8.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [7.0900e+02 4.2430e+03 1.9200e+03 ... 3.3310e+03 4.5660e+03 3.6580e+03]\n",
      " [7.4600e+02 2.5000e+01 1.4900e+02 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [1.2120e+03 5.3500e+02 9.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [1.4000e+01 8.5540e+03 9.0000e+01 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [3.3770e+03 5.0000e+00 1.0984e+04 ... 1.2000e+02 2.3700e+02 1.1000e+01]]\n",
      "<NDArray 1529x15 @cpu_shared(0)> \n",
      "[ 3 31  3 ... 35 31 10]\n",
      "<NDArray 1529 @cpu_shared(0)>\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in train_dataloader:\n",
    "    print(tweet, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model contruction\n",
    "Self attention layer, weighted cross entropy, and whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Block):\n",
    "    def __init__(self, vocab_len, embed_size, kernel_sizes, num_channels, \\\n",
    "                 dropout, nclass, **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_len, embed_size)\n",
    "        \n",
    "        self.constant_embedding = nn.Embedding(vocab_len, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.decoder = nn.Dense(nclass)\n",
    "        self.pool = nn.GlobalMaxPool1D()\n",
    "        self.convs = nn.Sequential()  \n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.add(nn.Conv1D(c, k, activation='relu'))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeddings = nd.concat(\n",
    "            self.embedding(inputs), self.constant_embedding(inputs), dim=2)\n",
    "        embeddings = embeddings.transpose((0, 2, 1))\n",
    "        \n",
    "        encoding = nd.concat(*[nd.flatten(\n",
    "            self.pool(conv(embeddings))) for conv in self.convs], dim=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embedding): Embedding(200004 -> 300, float32)\n",
      "  (constant_embedding): Embedding(200004 -> 300, float32)\n",
      "  (dropout): Dropout(p = 0.2, axes=())\n",
      "  (decoder): Dense(None -> 72, linear)\n",
      "  (pool): GlobalMaxPool1D(size=(1,), stride=(1,), padding=(0,), ceil_mode=True)\n",
      "  (convs): Sequential(\n",
      "    (0): Conv1D(None -> 100, kernel_size=(2,), stride=(1,), Activation(relu))\n",
      "    (1): Conv1D(None -> 100, kernel_size=(3,), stride=(1,), Activation(relu))\n",
      "    (2): Conv1D(None -> 100, kernel_size=(4,), stride=(1,), Activation(relu))\n",
      "    (3): Conv1D(None -> 100, kernel_size=(5,), stride=(1,), Activation(relu))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300   # word embedding size\n",
    "nhidden = 400    # lstm hidden_dim\n",
    "nlayers = 4     # lstm layers\n",
    "natt_unit = 400     # the hidden_units of attention layer\n",
    "natt_hops = 20    # the channels of attention\n",
    "nfc = 256  # last dense layer size\n",
    "nclass = 72 # we have 72 emoji in total\n",
    "\n",
    "drop_prob = 0.2\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "kernel_sizes, nums_channels = [2, 3, 4, 5], [100, 100, 100, 100]\n",
    "model = TextCNN(vocab_len, emsize, kernel_sizes, nums_channels, drop_prob, nclass)\n",
    "model.initialize(init.Xavier(), ctx=ctx)\n",
    "\n",
    "print(model)\n",
    "model.embedding.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "model.constant_embedding.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "model.constant_embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-1.4535143   0.4316371   0.03780118 -0.43824768  0.41816843 -1.0937376\n",
       "  -0.71099544  0.16095117 -0.2091109  -0.42367667  1.634491    0.38694513\n",
       "   1.4579254   1.0658729  -0.39729273  1.4154611   0.0408833   0.9625461\n",
       "   0.91877794  0.43661913 -0.19778824 -0.8435166  -1.9327714   0.90318596\n",
       "   0.75077903  0.7486939   1.0200762   1.2349734   2.3193169   1.1560069\n",
       "   1.1089306   0.6726989   0.659776   -0.91031754  0.99988955 -0.06935319\n",
       "  -1.7049602   0.74020696  1.9678237  -1.4590362  -1.4255147   1.3767172\n",
       "  -0.9265099   0.81759036 -0.7621071   0.97436655 -1.5960187  -0.15850382\n",
       "  -0.19329947 -0.36371315  0.3686909   0.14982761 -0.6566879  -0.92307013\n",
       "   0.7295457   0.3880864   1.8115426  -0.93350893 -0.9633518   1.2313509\n",
       "  -2.288724    0.3680066  -1.5167515   0.25544786  0.13798247  0.13479555\n",
       "  -0.78266764  0.54013383 -0.19405475 -0.70071375  1.7750703   1.1349797 ]]\n",
       "<NDArray 1x72 @gpu(0)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = nd.array([10, 20, 30, 40, 50, 60], ctx=ctx).reshape(1, -1)\n",
    "model(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, y, model, loss, class_weight):\n",
    "    pred = model(x)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name == 'sce':\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch,\n",
    "              clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x, batch_y in data_iter:\n",
    "        batch_x = batch_x.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                               loss, class_weight)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                           loss, class_weight)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 3 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs,\n",
    "                clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_weight\n",
    "weight_list = get_weight(DATA_FOLDER, LABEL_FILE)\n",
    "\n",
    "class_weight = None\n",
    "loss_name = 'sce'\n",
    "optim = 'adam'\n",
    "lr, wd = .001, .999\n",
    "clip = None\n",
    "nepochs = 5\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array(weight_list, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu(0)\n",
      "epoch 1, batch 400, batch_train_loss 3.4517, batch_train_acc 0.155\n",
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 3.5593, acc_train 0.152, F1_train 0.101, \n",
      "\t valid_loss 3.4231, acc_valid 0.170, F1_valid 0.106, \n",
      "time 105.61 sec\n",
      "****************************************************************************************************\n",
      "epoch 2, batch 400, batch_train_loss 3.3264, batch_train_acc 0.178\n",
      "epoch 2, learning_rate 0.00100 \n",
      "\t train_loss 3.3224, acc_train 0.181, F1_train 0.128, \n",
      "\t valid_loss 3.3578, acc_valid 0.179, F1_valid 0.125, \n",
      "time 106.17 sec\n",
      "****************************************************************************************************\n",
      "epoch 3, batch 400, batch_train_loss 3.2211, batch_train_acc 0.206\n",
      "epoch 3, learning_rate 0.00100 \n",
      "\t train_loss 3.1981, acc_train 0.200, F1_train 0.148, \n",
      "\t valid_loss 3.3231, acc_valid 0.186, F1_valid 0.135, \n",
      "time 106.75 sec\n",
      "****************************************************************************************************\n",
      "epoch 4, batch 400, batch_train_loss 3.1442, batch_train_acc 0.219\n",
      "epoch 4, learning_rate 0.00090 \n",
      "\t train_loss 3.0581, acc_train 0.224, F1_train 0.174, \n",
      "\t valid_loss 3.3449, acc_valid 0.182, F1_valid 0.133, \n",
      "time 106.78 sec\n",
      "****************************************************************************************************\n",
      "epoch 5, batch 400, batch_train_loss 3.0234, batch_train_acc 0.243\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "print(ctx)\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, clip=clip, class_weight=class_weight, \\\n",
    "            loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_parameters(\"model/textcnn.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes, nums_channels = [2, 3, 4, 5], [100, 100, 100, 100]\n",
    "model = TextCNN(vocab_len, emsize, kernel_sizes, nums_channels, 0, nclass)\n",
    "model.load_parameters('model/textcnn.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATA = 'test.csv'\n",
    "predictions = []\n",
    "test_df = pd.read_csv(DATA_FOLDER+TEST_DATA, header=None, sep='\\t')\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user_data/anaconda3/lib/python3.6/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmp2d02kovo' -> '/tmp/jieba.cache'\n",
      "ERROR:jieba:Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user_data/anaconda3/lib/python3.6/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmp2d02kovo' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.884 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.884 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pred len 2000, time 6.17s\n",
      "current pred len 4000, time 4.82s\n",
      "current pred len 6000, time 4.99s\n",
      "current pred len 8000, time 5.27s\n",
      "current pred len 10000, time 5.15s\n",
      "current pred len 12000, time 4.91s\n",
      "current pred len 14000, time 5.08s\n",
      "current pred len 16000, time 4.36s\n",
      "current pred len 18000, time 4.03s\n",
      "current pred len 20000, time 4.68s\n",
      "current pred len 22000, time 5.15s\n",
      "current pred len 24000, time 5.01s\n",
      "current pred len 26000, time 5.03s\n",
      "current pred len 28000, time 4.93s\n",
      "current pred len 30000, time 4.76s\n",
      "current pred len 32000, time 4.54s\n",
      "current pred len 34000, time 4.81s\n",
      "current pred len 36000, time 4.94s\n",
      "current pred len 38000, time 5.04s\n",
      "current pred len 40000, time 4.43s\n",
      "current pred len 42000, time 4.07s\n",
      "current pred len 44000, time 4.23s\n",
      "current pred len 46000, time 4.22s\n",
      "current pred len 48000, time 4.35s\n",
      "current pred len 50000, time 4.71s\n",
      "current pred len 52000, time 4.94s\n",
      "current pred len 54000, time 5.11s\n",
      "current pred len 56000, time 4.20s\n",
      "current pred len 58000, time 4.97s\n",
      "current pred len 60000, time 4.53s\n",
      "current pred len 62000, time 5.01s\n",
      "current pred len 64000, time 4.52s\n",
      "current pred len 66000, time 5.17s\n",
      "current pred len 68000, time 4.49s\n",
      "current pred len 70000, time 4.67s\n",
      "current pred len 72000, time 5.23s\n",
      "current pred len 74000, time 5.05s\n",
      "current pred len 76000, time 3.94s\n",
      "current pred len 78000, time 3.79s\n",
      "current pred len 80000, time 3.91s\n",
      "current pred len 82000, time 4.08s\n",
      "current pred len 84000, time 4.97s\n",
      "current pred len 86000, time 4.59s\n",
      "current pred len 88000, time 4.97s\n",
      "current pred len 90000, time 5.22s\n",
      "current pred len 92000, time 4.93s\n",
      "current pred len 94000, time 4.07s\n",
      "current pred len 96000, time 4.69s\n",
      "current pred len 98000, time 5.06s\n",
      "current pred len 100000, time 4.92s\n",
      "current pred len 102000, time 5.05s\n",
      "current pred len 104000, time 5.22s\n",
      "current pred len 106000, time 4.79s\n",
      "current pred len 108000, time 4.67s\n",
      "current pred len 110000, time 4.66s\n",
      "current pred len 112000, time 4.37s\n",
      "current pred len 114000, time 5.06s\n",
      "current pred len 116000, time 4.90s\n",
      "current pred len 118000, time 4.73s\n",
      "current pred len 120000, time 4.10s\n",
      "current pred len 122000, time 4.09s\n",
      "current pred len 124000, time 4.04s\n",
      "current pred len 126000, time 3.65s\n",
      "current pred len 128000, time 4.02s\n",
      "current pred len 130000, time 5.06s\n",
      "current pred len 132000, time 4.87s\n",
      "current pred len 134000, time 4.64s\n",
      "current pred len 136000, time 4.61s\n",
      "current pred len 138000, time 4.66s\n",
      "current pred len 140000, time 5.27s\n",
      "current pred len 142000, time 5.30s\n",
      "current pred len 144000, time 5.83s\n",
      "current pred len 146000, time 4.87s\n",
      "current pred len 148000, time 4.89s\n",
      "current pred len 150000, time 5.08s\n",
      "current pred len 152000, time 4.48s\n",
      "current pred len 154000, time 4.80s\n",
      "current pred len 156000, time 4.93s\n",
      "current pred len 158000, time 5.47s\n",
      "current pred len 160000, time 5.09s\n",
      "current pred len 162000, time 4.15s\n",
      "current pred len 164000, time 5.14s\n",
      "current pred len 166000, time 4.86s\n",
      "current pred len 168000, time 4.54s\n",
      "current pred len 170000, time 4.55s\n",
      "current pred len 172000, time 5.44s\n",
      "current pred len 174000, time 4.70s\n",
      "current pred len 176000, time 5.09s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for _, tweet in test_df.iterrows():\n",
    "    token = vocab[jieba.lcut(tweet[1])]\n",
    "    if len(token)<5:\n",
    "        token += [0.]*(5-len(token))\n",
    "    inp = nd.array(token, ctx=ctx).reshape(1,-1)\n",
    "    pred = model(inp)\n",
    "    pred = nd.argmax(pred, axis=1).asscalar()\n",
    "    predictions.append(int(pred))\n",
    "    if len(predictions)%2000==0:\n",
    "        ckpt = time.time()\n",
    "        print('current pred len %d, time %.2fs' % (len(predictions), ckpt-start))\n",
    "        start = ckpt\n",
    "submit = pd.DataFrame({'Expected': predictions})\n",
    "submit.to_csv('submission.csv', sep=',', index_label='ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
