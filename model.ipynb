{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import gluonnlp as nlp\n",
    "import jieba\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from d2l import try_gpu\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "WORD_EMBED = 'sgns.weibo.word' \n",
    "N_ROWS=50000\n",
    "ctx = try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_FOLDER+TRAIN_DATA, nrows=N_ROWS, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 10000)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =[ [row[0], row[1]] for _, row in train_df.iterrows()]\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, .2)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.218 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.218 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Loading model cost 1.261 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.261 seconds.\n",
      "Loading model cost 1.269 seconds.\n",
      "Loading model cost 1.306 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.269 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.306 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Loading model cost 1.331 seconds.\n",
      "Loading model cost 1.338 seconds.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Loading model cost 1.331 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.338 seconds.\n",
      "Loading model cost 1.345 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Loading model cost 1.345 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.348 seconds.\n",
      "Loading model cost 1.371 seconds.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Loading model cost 1.371 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.348 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.389 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Loading model cost 1.389 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Loading model cost 1.403 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.403 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Loading model cost 1.442 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.442 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=3.24s, #Sentences=40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.321 seconds.\n",
      "Loading model cost 1.334 seconds.\n",
      "Loading model cost 1.327 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.321 seconds.\n",
      "Loading model cost 1.335 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.334 seconds.\n",
      "Loading model cost 1.337 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Loading model cost 1.335 seconds.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Loading model cost 1.337 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Loading model cost 1.327 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.365 seconds.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Loading model cost 1.349 seconds.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Loading model cost 1.365 seconds.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Loading model cost 1.349 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Loading model cost 1.399 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.399 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.432 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.432 seconds.\n",
      "Loading model cost 1.444 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.444 seconds.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "Loading model cost 1.520 seconds.\n",
      "Loading model cost 1.540 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.520 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.540 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=2.63s, #Sentences=10000\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(x):\n",
    "    tweet, label = x\n",
    "    word_list = jieba.lcut(tweet)\n",
    "    return word_list, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def to_word_list(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(tokenizer, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_word_list, train_word_lengths = to_word_list(train_dataset)\n",
    "valid_word_list, valid_word_lengths = to_word_list(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/embedding/token_embedding.py:296: UserWarning: line 0 in data/sgns.weibo.word: skipped likely header line.\n",
      "  .format(line_num, pretrained_file_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=20004, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "train_seqs = [sample[0] for sample in train_word_list]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=20000)\n",
    "\n",
    "# load customed pre-trained embedding\n",
    "embedding_weights = nlp.embedding.TokenEmbedding.from_file(file_path=DATA_FOLDER+WORD_EMBED)\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_word_list)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=40000, batch_num=210\n",
      "  key=[18, 31, 44, 57, 70, 83, 96, 109, 122, 135]\n",
      "  cnt=[32991, 3840, 1480, 740, 491, 319, 120, 11, 5, 3]\n",
      "  batch_size=[240, 139, 98, 75, 64, 64, 64, 64, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # in this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_word_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[1.570e+03 5.000e+00 2.200e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [1.960e+02 5.160e+03 4.689e+03 ... 6.020e+02 8.000e+00 0.000e+00]\n",
      " [0.000e+00 6.830e+02 5.592e+03 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [3.897e+03 4.000e+00 1.910e+02 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [7.000e+00 1.600e+01 4.660e+02 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.135e+03 8.000e+00 7.600e+01 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "<NDArray 240x18 @cpu_shared(0)> \n",
      "[3 5 3 5 5 3 3 1 1 8 3 8 3 8 0 7 9 0 3 3 0 3 4 9 4 4 0 3 8 0 4 4 4 4 2 1 2\n",
      " 5 4 3 8 6 4 4 2 5 3 2 5 4 5 5 6 9 9 4 5 6 3 4 3 1 1 7 1 2 5 0 5 9 0 8 9 3\n",
      " 5 4 0 6 8 2 5 5 4 3 3 0 4 2 1 4 2 9 4 3 2 1 9 4 6 9 3 2 4 9 9 4 3 0 3 5 5\n",
      " 4 7 1 9 1 0 2 1 9 6 0 3 2 1 4 8 2 4 2 4 9 5 8 6 8 6 5 9 8 2 0 2 7 8 9 3 7\n",
      " 9 4 1 1 5 9 5 7 5 0 1 7 9 8 0 3 3 6 8 3 4 3 2 7 3 9 3 9 2 3 9 4 7 3 5 3 2\n",
      " 2 5 6 3 5 4 0 5 5 3 4 2 4 6 9 4 5 9 8 1 2 2 4 8 2 2 5 8 4 8 3 3 8 0 9 1 9\n",
      " 5 8 4 4 7 6 5 7 3 1 5 5 5 1 0 7 4 4]\n",
      "<NDArray 240 @cpu_shared(0)>\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in train_dataloader:\n",
    "    print(tweet, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model contruction\n",
    "Self attention layer, weighted cross entropy, and whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention(\n",
      "  (ut_dense): Dense(None -> 20, Activation(tanh))\n",
      "  (et_dense): Dense(None -> 5, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# custom attention layer\n",
    "# in this class, we want to implement the operation:\n",
    "# softmax(W_2 * tanh(W_1 * H))\n",
    "# where H is the word embedding of the whole sentence, of shape (num_of_word, embed_size)\n",
    "class SelfAttention(nn.HybridBlock):\n",
    "    def __init__(self, att_unit, att_hops, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # this layer is tanh(w_1 * H), the att_unit corresponds to d_a in the essay\n",
    "            self.ut_dense = nn.Dense(att_unit, activation='tanh', flatten=False)\n",
    "            # this layer implements the multiple hops\n",
    "            self.et_dense = nn.Dense(att_hops, activation=None, flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x): # F is the backend which implements the tensor operation\n",
    "        # x shape: [batch_size, seq_len, embedding_width]\n",
    "        # ut shape: [batch_size, seq_len, att_unit]\n",
    "        ut = self.ut_dense(x) # batch_size * seq_len [* embed_size * embed_size *] att_unit\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)# batch_size * seq_len [* att_unit * att_unit *] att_hops\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        # softmax is performed along the seq_len dimension\n",
    "        att = F.softmax(F.transpose(et, axes=(0, 2, 1)), axis=-1)\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = F.batch_dot(att, x)\n",
    "        # output is the weighted matrix representation of the matrix\n",
    "        # att is the weighted vector we use as attention\n",
    "        return output, att\n",
    "    \n",
    "# d_a = 20, hops = 5\n",
    "print(SelfAttention(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentiveBiLSTM(nn.HybridBlock):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, natt_unit, natt_hops, \\\n",
    "                 nfc, nclass, # these two params are not used currrently\n",
    "                 drop_prob, pool_way, prune_p=None, prune_q=None, **kwargs):\n",
    "        super(SelfAttentiveBiLSTM, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # now we switch back to shared layers\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            \n",
    "            self.bilstm = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, \\\n",
    "                                        bidirectional=True)\n",
    "            \n",
    "            self.att_encoder = SelfAttention(natt_unit, natt_hops)\n",
    "            self.dense = nn.Dense(nfc, activation='tanh')\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.Dense(nclass)\n",
    "            \n",
    "            self.dense_p, self.dense_q = None, None\n",
    "            if all([prune_p, prune_q]):\n",
    "                self.dense_p = nn.Dense(prune_p, activation='tanh', flatten=False)\n",
    "                self.dense_q = nn.Dense(prune_q, activation='tanh', flatten=False)\n",
    "\n",
    "            self.drop_prob = drop_prob\n",
    "            self.pool_way = pool_way\n",
    "\n",
    "    def hybrid_forward(self, F, inp):\n",
    "        # inp_embed size: [batch, seq_len, embed_size]\n",
    "        inp_embed = self.embedding_layer(inp)\n",
    "        # rnn requires the first dimension to be the time steps\n",
    "        h_output = self.bilstm(F.transpose(inp_embed, axes=(1, 0, 2)))\n",
    "        # att_output: [batch, att_hops, emsize]\n",
    "        output, att = self.att_encoder(F.transpose(h_output, axes=(1, 0, 2)))\n",
    "        '''\n",
    "        FIXME: now this code will only work with flatten\n",
    "        '''\n",
    "        dense_input = None\n",
    "        if self.pool_way == 'flatten':\n",
    "            dense_input = F.Dropout(F.flatten(output), self.drop_prob)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        '''\n",
    "        elif self.pool_way == 'mean':\n",
    "            dense_input = F.Dropout(F.mean(att_output, axis=1), self.drop_prob)\n",
    "        elif self.pool_way == 'prune' and all([self.dense_p, self.dense_q]):\n",
    "            # p_section: [batch, att_hops, prune_p]\n",
    "            p_section = self.dense_p(att_output)\n",
    "            # q_section: [batch, emsize, prune_q]\n",
    "            q_section = self.dense_q(F.transpose(att_output, axes=(0, 2, 1)))\n",
    "            dense_input = F.Dropout(F.concat(F.flatten(p_section), F.flatten(q_section), dim=-1), self.drop_prob)\n",
    "        '''\n",
    "        dense_out = self.dense(dense_input)\n",
    "        output = self.output_layer(F.Dropout(dense_out, self.drop_prob))\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttentiveBiLSTM(\n",
      "  (embedding_layer): Embedding(20004 -> 300, float32)\n",
      "  (bilstm): LSTM(None -> 300, TNC, num_layers=2, bidirectional)\n",
      "  (att_encoder): SelfAttention(\n",
      "    (ut_dense): Dense(None -> 300, Activation(tanh))\n",
      "    (et_dense): Dense(None -> 20, linear)\n",
      "  )\n",
      "  (dense): Dense(None -> 512, Activation(tanh))\n",
      "  (output_layer): Dense(None -> 72, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300   # word embedding size\n",
    "nhidden = 300    # lstm hidden_dim\n",
    "nlayers = 2     # lstm layers\n",
    "natt_unit = 300     # the hidden_units of attention layer\n",
    "natt_hops = 20    # the channels of attention\n",
    "nfc = 512  # last dense layer size\n",
    "nclass = 72 # we have 72 emoji in total\n",
    "\n",
    "drop_prob = 0\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "model = SelfAttentiveBiLSTM(vocab_len, emsize, nhidden, nlayers,\n",
    "                            natt_unit, natt_hops, nfc, nclass,\n",
    "                            drop_prob, pool_way, prune_p, prune_q)\n",
    "\n",
    "model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "# Attach a pre-trained glove word vector to the embedding layer\n",
    "model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "# fixed the embedding layer\n",
    "model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [-1.10227e-01  1.08770e-01  4.29162e-01 -2.35640e-01  1.42376e-01\n",
       "   2.28883e-01  2.58560e-02  2.00537e-01  1.75788e-01  3.71191e-01\n",
       "   3.62292e-01 -4.53204e-01  2.59187e-01 -2.68126e-01 -4.26229e-01\n",
       "  -4.58031e-01 -2.79040e-01  2.77700e-03  3.84063e-01  1.95392e-01\n",
       "   2.52946e-01 -8.82490e-02  6.61550e-02  6.06530e-02 -1.81174e-01\n",
       "   1.03426e-01 -2.29491e-01  1.63837e-01 -1.18820e-02 -2.51057e-01\n",
       "   3.30090e-02 -1.70570e-02 -2.50483e-01  8.84330e-02  3.87295e-01\n",
       "  -8.91180e-02 -1.68160e-02 -7.72610e-02 -1.49332e-01 -3.09141e-01\n",
       "  -1.70612e-01  2.17445e-01 -3.46855e-01 -6.69050e-02  2.34772e-01\n",
       "  -8.18120e-02  6.82540e-01  3.94989e-01 -2.64939e-01 -1.54585e-01\n",
       "   3.50940e-02 -8.46420e-02 -1.83615e-01  2.74247e-01 -3.89414e-01\n",
       "   2.04440e-02  2.28073e-01  9.59450e-02  2.27668e-01  2.52022e-01\n",
       "   1.24785e-01  2.00795e-01 -1.37927e-01 -2.09281e-01  9.78250e-02\n",
       "  -8.04370e-02  2.25407e-01  3.29207e-01  9.87380e-02 -1.78504e-01\n",
       "  -3.52110e-02 -2.15054e-01 -1.65906e-01  1.03336e-01  5.42230e-02\n",
       "  -1.96891e-01  2.15154e-01 -1.66890e-02  1.63620e-02 -3.03190e-01\n",
       "  -3.56706e-01  2.36391e-01 -1.11360e-02  2.91456e-01 -1.05539e-01\n",
       "   6.70880e-02 -1.36463e-01  1.98300e-02 -1.56240e-01 -1.80833e-01\n",
       "  -1.42981e-01  3.06824e-01  1.41550e-01  4.24456e-01  3.34449e-01\n",
       "   1.06193e-01  1.06502e-01  3.17735e-01 -2.20299e-01 -2.46564e-01\n",
       "  -2.37252e-01  2.70377e-01 -4.58750e-02 -2.97061e-01 -1.99651e-01\n",
       "  -2.86634e-01 -4.93520e-02  2.01355e-01  4.73440e-02  2.59310e-01\n",
       "   7.17990e-02 -8.91397e-01  1.09855e-01  1.77070e-02  4.05341e-01\n",
       "  -1.88013e-01  1.06506e-01  6.84020e-02 -8.48960e-02 -8.54150e-02\n",
       "  -2.23373e-01  6.56750e-02  2.34492e-01 -5.50740e-01 -4.07130e-01\n",
       "  -2.34930e-02 -8.12540e-02 -3.09397e-01 -4.58661e-01  2.67103e-01\n",
       "  -1.45149e-01  2.03261e-01 -3.70510e-02  4.96520e-02 -5.27600e-03\n",
       "   1.26712e-01 -2.03743e-01 -2.22020e-02  1.83293e-01 -3.39342e-01\n",
       "   8.48070e-02  9.77270e-02 -2.53487e-01 -1.26112e-01  1.00737e-01\n",
       "  -1.18296e-01  3.34295e-01  8.79400e-02  4.70812e-01  1.47242e-01\n",
       "   4.63942e-01  4.61091e-01 -1.52714e-01 -8.72420e-02  6.16160e-02\n",
       "  -1.80841e-01  3.29998e-01  2.57331e-01 -1.21793e-01 -4.05468e-01\n",
       "  -4.37494e-01 -1.18967e-01 -2.61000e-04 -6.92260e-02  2.56550e-02\n",
       "  -7.61560e-02  1.80850e-01  2.77124e-01  2.60624e-01  1.60250e-02\n",
       "   1.81480e-02 -5.61840e-02  1.71342e-01  1.71090e-01  3.92180e-02\n",
       "  -2.02890e-02  1.92353e-01 -9.76110e-02  1.07765e-01 -3.14708e-01\n",
       "   3.00331e-01 -9.30220e-02 -1.58826e-01 -1.84878e-01  5.44669e-01\n",
       "   6.24860e-02  8.32268e-01  3.22727e-01 -2.64234e-01 -1.93605e-01\n",
       "   1.72048e-01  1.12664e-01  1.97914e-01  4.61738e-01  8.34310e-02\n",
       "   9.15710e-02  2.50279e-01 -1.35822e-01 -2.22395e-01 -4.71872e-01\n",
       "   5.84460e-02  1.06354e-01  4.93620e-02 -1.67192e-01  2.41342e-01\n",
       "  -1.18352e-01 -1.73532e-01  1.09683e-01  1.96661e-01 -3.08209e-01\n",
       "   5.16000e-04 -1.37176e-01  4.99902e-01  1.58656e-01 -4.65326e-01\n",
       "   1.88111e-01  2.53838e-01 -6.02850e-02 -2.64138e-01 -9.63970e-02\n",
       "   1.48640e-01  3.88053e-01 -4.79879e-01  1.08633e-01  2.82220e-02\n",
       "  -1.01100e-03  1.15165e-01 -4.37100e-02  1.08400e-02  1.97620e-01\n",
       "   2.59160e-02  1.56250e-02 -1.76083e-01  1.24611e-01 -2.79410e-02\n",
       "  -4.21010e-02 -5.15090e-02  3.75256e-01  3.97390e-02  1.85160e-02\n",
       "  -9.30750e-02 -3.92520e-02 -1.64304e-01 -4.62800e-01 -1.31523e-01\n",
       "   1.62289e-01 -2.83872e-01 -4.05836e-01 -1.16554e-01 -2.79717e-01\n",
       "  -1.23506e-01  3.29233e-01 -7.30392e-01 -9.10210e-02  3.23390e-02\n",
       "   3.18455e-01  4.30191e-01  5.08040e-02 -4.47720e-01 -8.26070e-02\n",
       "  -2.96930e-02 -3.10748e-01  1.79804e-01  3.36771e-01 -2.87940e-02\n",
       "   2.51968e-01 -9.87790e-02 -1.39913e-01  1.48674e-01 -2.62751e-01\n",
       "   4.10888e-01 -2.13344e-01  3.03260e-02  1.84527e-01 -7.17250e-02\n",
       "  -2.03044e-01  3.13409e-01 -2.33795e-01 -4.28000e-04  6.35920e-02\n",
       "  -2.67452e-01 -4.57820e-02 -4.50190e-02 -1.66526e-01 -4.22746e-01\n",
       "   3.98445e-01 -2.98592e-01  3.26730e-02  3.17010e-02 -3.12050e-02\n",
       "  -1.96815e-01  1.23919e-01 -6.43750e-02 -7.12380e-02 -1.23865e-01\n",
       "  -1.44593e-01  1.87790e-02  3.27644e-01  4.12230e-02 -1.48440e-02]\n",
       " <NDArray 300 @cpu(0)>, 121)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding.idx_to_vec[15], vocab.embedding.token_to_idx['用']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helpers\n",
    "Calculate loss, one epoch computation and top function for train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, y, model, loss, class_weight, penal_coeff):\n",
    "    pred, att = model(x)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name in ['sce', 'l1', 'l2']:\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    # penalty, now we have two att's\n",
    "    diversity_penalty = nd.batch_dot(att, nd.transpose(att, axes=(0, 2, 1))) - \\\n",
    "                        nd.eye(att.shape[1], ctx=att.context)\n",
    "    l = l + penal_coeff * diversity_penalty.norm(axis=(1, 2))\n",
    "\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch,\n",
    "              penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x, batch_y in data_iter:\n",
    "        batch_x = batch_x.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                               loss, class_weight, penal_coeff)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                           loss, class_weight, penal_coeff)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 3 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs,\n",
    "                penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now we will train this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "loss_name = 'sce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "penal_coeff = 0.003\n",
    "clip = .5\n",
    "nepochs = 40\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array([1., 3.], ctx=ctx)\n",
    "elif loss_name == 'l1':\n",
    "    loss = gluon.loss.L1Loss()\n",
    "elif loss_name == 'l2':\n",
    "    loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 2.7377, acc_train 0.142, F1_train 0.093, \n",
      "\t valid_loss 2.5979, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.68 sec\n",
      "****************************************************************************************************\n",
      "epoch 2, learning_rate 0.00100 \n",
      "\t train_loss 2.8613, acc_train 0.127, F1_train 0.092, \n",
      "\t valid_loss 2.2557, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.56 sec\n",
      "****************************************************************************************************\n",
      "epoch 3, learning_rate 0.00100 \n",
      "\t train_loss 2.4439, acc_train 0.143, F1_train 0.094, \n",
      "\t valid_loss 3.8958, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.58 sec\n",
      "****************************************************************************************************\n",
      "epoch 4, learning_rate 0.00090 \n",
      "\t train_loss 3.8203, acc_train 0.133, F1_train 0.092, \n",
      "\t valid_loss 2.3923, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.57 sec\n",
      "****************************************************************************************************\n",
      "epoch 5, learning_rate 0.00090 \n",
      "\t train_loss 2.7067, acc_train 0.130, F1_train 0.083, \n",
      "\t valid_loss 2.2983, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.65 sec\n",
      "****************************************************************************************************\n",
      "epoch 6, learning_rate 0.00090 \n",
      "\t train_loss 2.9063, acc_train 0.143, F1_train 0.092, \n",
      "\t valid_loss 2.2524, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.66 sec\n",
      "****************************************************************************************************\n",
      "epoch 7, learning_rate 0.00081 \n",
      "\t train_loss 2.7685, acc_train 0.143, F1_train 0.085, \n",
      "\t valid_loss 3.7326, acc_valid 0.061, F1_valid 0.007, \n",
      "time 15.86 sec\n",
      "****************************************************************************************************\n",
      "epoch 8, learning_rate 0.00081 \n",
      "\t train_loss 2.9132, acc_train 0.121, F1_train 0.092, \n",
      "\t valid_loss 2.2652, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.63 sec\n",
      "****************************************************************************************************\n",
      "epoch 9, learning_rate 0.00081 \n",
      "\t train_loss 2.8646, acc_train 0.146, F1_train 0.097, \n",
      "\t valid_loss 2.2770, acc_valid 0.149, F1_valid 0.039, \n",
      "time 15.69 sec\n",
      "****************************************************************************************************\n",
      "epoch 10, learning_rate 0.00073 \n",
      "\t train_loss 2.6707, acc_train 0.136, F1_train 0.084, \n",
      "\t valid_loss 2.3151, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.66 sec\n",
      "****************************************************************************************************\n",
      "epoch 11, learning_rate 0.00073 \n",
      "\t train_loss 2.6282, acc_train 0.147, F1_train 0.088, \n",
      "\t valid_loss 2.5040, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.64 sec\n",
      "****************************************************************************************************\n",
      "epoch 12, learning_rate 0.00073 \n",
      "\t train_loss 2.6687, acc_train 0.147, F1_train 0.082, \n",
      "\t valid_loss 2.8668, acc_valid 0.061, F1_valid 0.007, \n",
      "time 15.67 sec\n",
      "****************************************************************************************************\n",
      "epoch 13, learning_rate 0.00066 \n",
      "\t train_loss 2.6631, acc_train 0.136, F1_train 0.084, \n",
      "\t valid_loss 2.2566, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.58 sec\n",
      "****************************************************************************************************\n",
      "epoch 14, learning_rate 0.00066 \n",
      "\t train_loss 2.6318, acc_train 0.153, F1_train 0.085, \n",
      "\t valid_loss 3.6217, acc_valid 0.100, F1_valid 0.018, \n",
      "time 15.63 sec\n",
      "****************************************************************************************************\n",
      "epoch 15, learning_rate 0.00066 \n",
      "\t train_loss 2.8102, acc_train 0.151, F1_train 0.083, \n",
      "\t valid_loss 2.2661, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.68 sec\n",
      "****************************************************************************************************\n",
      "epoch 16, learning_rate 0.00059 \n",
      "\t train_loss 2.6404, acc_train 0.146, F1_train 0.081, \n",
      "\t valid_loss 2.3794, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.66 sec\n",
      "****************************************************************************************************\n",
      "epoch 17, learning_rate 0.00059 \n",
      "\t train_loss 2.5851, acc_train 0.150, F1_train 0.072, \n",
      "\t valid_loss 2.2534, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.70 sec\n",
      "****************************************************************************************************\n",
      "epoch 18, learning_rate 0.00059 \n",
      "\t train_loss 2.5162, acc_train 0.148, F1_train 0.084, \n",
      "\t valid_loss 2.5496, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.69 sec\n",
      "****************************************************************************************************\n",
      "epoch 19, learning_rate 0.00053 \n",
      "\t train_loss 2.5450, acc_train 0.150, F1_train 0.081, \n",
      "\t valid_loss 2.2948, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.74 sec\n",
      "****************************************************************************************************\n",
      "epoch 20, learning_rate 0.00053 \n",
      "\t train_loss 2.3920, acc_train 0.153, F1_train 0.077, \n",
      "\t valid_loss 3.5332, acc_valid 0.100, F1_valid 0.018, \n",
      "time 15.69 sec\n",
      "****************************************************************************************************\n",
      "epoch 21, learning_rate 0.00053 \n",
      "\t train_loss 2.6072, acc_train 0.164, F1_train 0.076, \n",
      "\t valid_loss 2.2566, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.89 sec\n",
      "****************************************************************************************************\n",
      "epoch 22, learning_rate 0.00048 \n",
      "\t train_loss 2.4837, acc_train 0.149, F1_train 0.084, \n",
      "\t valid_loss 2.2528, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.70 sec\n",
      "****************************************************************************************************\n",
      "epoch 23, learning_rate 0.00048 \n",
      "\t train_loss 2.4460, acc_train 0.145, F1_train 0.074, \n",
      "\t valid_loss 2.2453, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.70 sec\n",
      "****************************************************************************************************\n",
      "epoch 24, learning_rate 0.00048 \n",
      "\t train_loss 2.5082, acc_train 0.140, F1_train 0.089, \n",
      "\t valid_loss 2.6723, acc_valid 0.061, F1_valid 0.007, \n",
      "time 15.76 sec\n",
      "****************************************************************************************************\n",
      "epoch 25, learning_rate 0.00043 \n",
      "\t train_loss 2.6074, acc_train 0.154, F1_train 0.078, \n",
      "\t valid_loss 2.2479, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.78 sec\n",
      "****************************************************************************************************\n",
      "epoch 26, learning_rate 0.00043 \n",
      "\t train_loss 2.3748, acc_train 0.159, F1_train 0.059, \n",
      "\t valid_loss 3.3592, acc_valid 0.100, F1_valid 0.018, \n",
      "time 15.69 sec\n",
      "****************************************************************************************************\n",
      "epoch 27, learning_rate 0.00043 \n",
      "\t train_loss 2.4308, acc_train 0.156, F1_train 0.080, \n",
      "\t valid_loss 3.3305, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.68 sec\n",
      "****************************************************************************************************\n",
      "epoch 28, learning_rate 0.00039 \n",
      "\t train_loss 2.5022, acc_train 0.139, F1_train 0.088, \n",
      "\t valid_loss 2.2745, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.68 sec\n",
      "****************************************************************************************************\n",
      "epoch 29, learning_rate 0.00039 \n",
      "\t train_loss 2.3879, acc_train 0.153, F1_train 0.066, \n",
      "\t valid_loss 2.2420, acc_valid 0.149, F1_valid 0.039, \n",
      "time 15.72 sec\n",
      "****************************************************************************************************\n",
      "epoch 30, learning_rate 0.00039 \n",
      "\t train_loss 2.3816, acc_train 0.157, F1_train 0.072, \n",
      "\t valid_loss 2.2410, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.74 sec\n",
      "****************************************************************************************************\n",
      "epoch 31, learning_rate 0.00035 \n",
      "\t train_loss 2.3422, acc_train 0.155, F1_train 0.065, \n",
      "\t valid_loss 2.4422, acc_valid 0.100, F1_valid 0.018, \n",
      "time 15.75 sec\n",
      "****************************************************************************************************\n",
      "epoch 32, learning_rate 0.00035 \n",
      "\t train_loss 2.3742, acc_train 0.157, F1_train 0.079, \n",
      "\t valid_loss 2.2447, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.91 sec\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33, learning_rate 0.00035 \n",
      "\t train_loss 2.3522, acc_train 0.158, F1_train 0.072, \n",
      "\t valid_loss 2.2862, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.78 sec\n",
      "****************************************************************************************************\n",
      "epoch 34, learning_rate 0.00031 \n",
      "\t train_loss 2.3597, acc_train 0.142, F1_train 0.073, \n",
      "\t valid_loss 2.2412, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.68 sec\n",
      "****************************************************************************************************\n",
      "epoch 35, learning_rate 0.00031 \n",
      "\t train_loss 2.3695, acc_train 0.150, F1_train 0.064, \n",
      "\t valid_loss 2.2397, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.73 sec\n",
      "****************************************************************************************************\n",
      "epoch 36, learning_rate 0.00031 \n",
      "\t train_loss 2.3329, acc_train 0.164, F1_train 0.067, \n",
      "\t valid_loss 2.8890, acc_valid 0.100, F1_valid 0.018, \n",
      "time 15.74 sec\n",
      "****************************************************************************************************\n",
      "epoch 37, learning_rate 0.00028 \n",
      "\t train_loss 2.4089, acc_train 0.157, F1_train 0.076, \n",
      "\t valid_loss 2.2479, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.24 sec\n",
      "****************************************************************************************************\n",
      "epoch 38, learning_rate 0.00028 \n",
      "\t train_loss 2.3442, acc_train 0.145, F1_train 0.073, \n",
      "\t valid_loss 2.2402, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.53 sec\n",
      "****************************************************************************************************\n",
      "epoch 39, learning_rate 0.00028 \n",
      "\t train_loss 2.3678, acc_train 0.155, F1_train 0.070, \n",
      "\t valid_loss 2.2413, acc_valid 0.176, F1_valid 0.052, \n",
      "time 15.63 sec\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, penal_coeff=penal_coeff, \\\n",
    "            clip=clip, class_weight=class_weight, loss_name=loss_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
