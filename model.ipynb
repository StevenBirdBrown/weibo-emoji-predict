{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import gluonnlp as nlp\n",
    "import jieba\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from d2l import try_gpu\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "WORD_EMBED = 'sgns.weibo.word'\n",
    "LABEL_FILE = 'train.label'\n",
    "N_ROWS=50000\n",
    "ctx = try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_FOLDER+TRAIN_DATA, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690531, 172633)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =[ [row[0], row[1]] for _, row in train_df.iterrows()]\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, .2)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.170 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.260 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.262 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.262 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.341 seconds.\n",
      "Loading model cost 1.430 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.324 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.321 seconds.\n",
      "Loading model cost 1.298 seconds.\n",
      "Loading model cost 1.372 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.435 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.533 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=19.46s, #Sentences=690531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.195 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.173 seconds.\n",
      "Loading model cost 1.199 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.198 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.238 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.331 seconds.\n",
      "Loading model cost 1.288 seconds.\n",
      "Loading model cost 1.258 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.337 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.314 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.332 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.509 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=6.19s, #Sentences=172633\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(x):\n",
    "    tweet, label = x\n",
    "    if type(tweet) != str:\n",
    "        tweet = str(tweet)\n",
    "    word_list = jieba.lcut(tweet)\n",
    "    return word_list, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def to_word_list(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(tokenizer, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_word_list, train_word_lengths = to_word_list(train_dataset)\n",
    "valid_word_list, valid_word_lengths = to_word_list(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/embedding/token_embedding.py:296: UserWarning: line 0 in data/sgns.weibo.word: skipped likely header line.\n",
      "  .format(line_num, pretrained_file_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=40004, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "train_seqs = [sample[0] for sample in train_word_list]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=40000)\n",
    "\n",
    "# load customed pre-trained embedding\n",
    "embedding_weights = nlp.embedding.TokenEmbedding.from_file(file_path=DATA_FOLDER+WORD_EMBED)\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_word_list)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=690531, batch_num=1649\n",
      "  key=[27, 47, 67, 87, 107, 127, 147, 167, 187, 207]\n",
      "  cnt=[592849, 59408, 23322, 11861, 2847, 166, 47, 17, 7, 7]\n",
      "  batch_size=[490, 281, 197, 152, 128, 128, 128, 128, 128, 128]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # in this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_word_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[9.0950e+03 9.1750e+03 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [1.2007e+04 6.0000e+00 2.0830e+03 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [7.8350e+03 1.8485e+04 6.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [0.0000e+00 1.0000e+01 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [8.0000e+00 2.7000e+01 1.2264e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [7.3900e+02 5.0000e+00 4.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]\n",
      "<NDArray 490x27 @cpu_shared(0)> \n",
      "[10  3 68 25 50  6 66 30 59 43 30 49 52 18 35 35 38 15 63 19 19 27 67 35\n",
      " 28 61 31 33 65 63 62 31 60 12 34  8 42 57 35 23 52 30 31 34 16 12  3 42\n",
      " 23 54 34 31 23 34 23 10 45 44  3 12 23 34 40 13 23 27  3  3 30 20 62  1\n",
      "  3 62 43 35 32 29  6 14 44 21 14 34 45  3 45 52 30 54 34 10 27  9 44 19\n",
      " 35  6 14 23  9 35 21  3  3 14 16 46 48 34 30 35 62  7 16  3  3 35 18 50\n",
      "  1 41 35 35 30 55 27  1 34 52 42  3 24  3 14 35 30 65 39 29 18 31 41 49\n",
      " 56 71 17 27 35 15 28  6 39 30 23 35  3 50 18 29 12 62 35 47 14 12 41 53\n",
      " 10 29 29 16 41  4  3  1 60 12 34  3 50 16 16 15 10 20 61 41 29 39 20  3\n",
      " 31 16 15 26 50 17 22 42  3 19 47 14 29 36 31 44 15 40 14 35  8 34 15 45\n",
      " 64 35 35 52 37 35 23 22 14 56 15 60 23 60 14 52 35 14 27 68 12 62 27  3\n",
      " 34 16 10 57 31 39 35 52 35 48 23 17 31 29 46  1 23  3 29 12 31 55 14 12\n",
      " 15 29 31 17 29 55 44 23 29  8 34 55 28 52 43 20  9 52 23 35 34 15  3  3\n",
      " 26 30 15 14 35  3 26 27 16 35 58  9 35 35 17  3 10  3  3 23 60 12  3  3\n",
      " 64 29  3 29 40 29 18  1 29 58  2 42 41 28 35 36 35 35  6 54 12 40 10 14\n",
      " 14 34  3 35 14 45 19 23 34 18 14 31 20 11 12 39 14 23 31 23 35 10  3 35\n",
      " 29 30 34 15 36 60 31 29 23  8  1 39 12  6  6  1 23 30 43 14 70 46 27 23\n",
      " 30 16 34 45 26  1 44  9 27 28 30 12 31 35 14 12 15 12 58 35 12 64 19 58\n",
      " 37 31 19 25 31 28  1  1  9 26 15 18 40 30 27 40 65 39 24 18 23 29 34 35\n",
      " 16 27 34 17 19 42 31 14 34  1 15 35  3 27 14 31  9 35 34 35 21  3 15 14\n",
      " 44 23 29 46 23 35 12 35 29 50 12  3 14 23 52 28 68 37 18 32 35 18 68 43\n",
      " 40  3 42 34 42 30 19 49 11 52]\n",
      "<NDArray 490 @cpu_shared(0)>\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in train_dataloader:\n",
    "    print(tweet, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model contruction\n",
    "Self attention layer, weighted cross entropy, and whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention(\n",
      "  (ut_dense): Dense(None -> 20, Activation(tanh))\n",
      "  (et_dense): Dense(None -> 5, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# custom attention layer\n",
    "# in this class, we want to implement the operation:\n",
    "# softmax(W_2 * tanh(W_1 * H))\n",
    "# where H is the word embedding of the whole sentence, of shape (num_of_word, embed_size)\n",
    "class SelfAttention(nn.HybridBlock):\n",
    "    def __init__(self, att_unit, att_hops, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # this layer is tanh(w_1 * H), the att_unit corresponds to d_a in the essay\n",
    "            self.ut_dense = nn.Dense(att_unit, activation='tanh', flatten=False)\n",
    "            # this layer implements the multiple hops\n",
    "            self.et_dense = nn.Dense(att_hops, activation=None, flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x): # F is the backend which implements the tensor operation\n",
    "        # x shape: [batch_size, seq_len, embedding_width]\n",
    "        # ut shape: [batch_size, seq_len, att_unit]\n",
    "        ut = self.ut_dense(x) # batch_size * seq_len [* embed_size * embed_size *] att_unit\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)# batch_size * seq_len [* att_unit * att_unit *] att_hops\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        # softmax is performed along the seq_len dimension\n",
    "        att = F.softmax(F.transpose(et, axes=(0, 2, 1)), axis=-1)\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = F.batch_dot(att, x)\n",
    "        # output is the weighted matrix representation of the matrix\n",
    "        # att is the weighted vector we use as attention\n",
    "        return output, att\n",
    "    \n",
    "# d_a = 20, hops = 5\n",
    "print(SelfAttention(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentiveBiLSTM(nn.HybridBlock):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, natt_unit, natt_hops, \\\n",
    "                 nfc, nclass, # these two params are not used currrently\n",
    "                 drop_prob, pool_way, prune_p=None, prune_q=None, **kwargs):\n",
    "        super(SelfAttentiveBiLSTM, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # now we switch back to shared layers\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            \n",
    "            self.bilstm = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, \\\n",
    "                                        bidirectional=True)\n",
    "            \n",
    "            self.att_encoder = SelfAttention(natt_unit, natt_hops)\n",
    "            self.dense = nn.Dense(nfc, activation='tanh')\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.Dense(nclass)\n",
    "            \n",
    "            self.dense_p, self.dense_q = None, None\n",
    "            if all([prune_p, prune_q]):\n",
    "                self.dense_p = nn.Dense(prune_p, activation='tanh', flatten=False)\n",
    "                self.dense_q = nn.Dense(prune_q, activation='tanh', flatten=False)\n",
    "\n",
    "            self.drop_prob = drop_prob\n",
    "            self.pool_way = pool_way\n",
    "\n",
    "    def hybrid_forward(self, F, inp):\n",
    "        # inp_embed size: [batch, seq_len, embed_size]\n",
    "        inp_embed = self.embedding_layer(inp)\n",
    "        # rnn requires the first dimension to be the time steps\n",
    "        h_output = self.bilstm(F.transpose(inp_embed, axes=(1, 0, 2)))\n",
    "        # att_output: [batch, att_hops, emsize]\n",
    "        output, att = self.att_encoder(F.transpose(h_output, axes=(1, 0, 2)))\n",
    "        '''\n",
    "        FIXME: now this code will only work with flatten\n",
    "        '''\n",
    "        dense_input = None\n",
    "        if self.pool_way == 'flatten':\n",
    "            dense_input = F.Dropout(F.flatten(output), self.drop_prob)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        '''\n",
    "        elif self.pool_way == 'mean':\n",
    "            dense_input = F.Dropout(F.mean(att_output, axis=1), self.drop_prob)\n",
    "        elif self.pool_way == 'prune' and all([self.dense_p, self.dense_q]):\n",
    "            # p_section: [batch, att_hops, prune_p]\n",
    "            p_section = self.dense_p(att_output)\n",
    "            # q_section: [batch, emsize, prune_q]\n",
    "            q_section = self.dense_q(F.transpose(att_output, axes=(0, 2, 1)))\n",
    "            dense_input = F.Dropout(F.concat(F.flatten(p_section), F.flatten(q_section), dim=-1), self.drop_prob)\n",
    "        '''\n",
    "        dense_out = self.dense(dense_input)\n",
    "        output = self.output_layer(F.Dropout(dense_out, self.drop_prob))\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize a new model\n",
      "SelfAttentiveBiLSTM(\n",
      "  (embedding_layer): Embedding(40004 -> 300, float32)\n",
      "  (bilstm): LSTM(None -> 300, TNC, num_layers=3, bidirectional)\n",
      "  (att_encoder): SelfAttention(\n",
      "    (ut_dense): Dense(None -> 300, Activation(tanh))\n",
      "    (et_dense): Dense(None -> 10, linear)\n",
      "  )\n",
      "  (dense): Dense(None -> 256, Activation(tanh))\n",
      "  (output_layer): Dense(None -> 72, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300   # word embedding size\n",
    "nhidden = 300    # lstm hidden_dim\n",
    "nlayers = 3     # lstm layers\n",
    "natt_unit = 300     # the hidden_units of attention layer\n",
    "natt_hops = 10    # the channels of attention\n",
    "nfc = 256  # last dense layer size\n",
    "nclass = 72 # we have 72 emoji in total\n",
    "\n",
    "drop_prob = 0\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "try:\n",
    "    assert(False)\n",
    "    model = gluon.nn.SymbolBlock.imports(\"model/model-symbol.json\", ['data'], \\\n",
    "                                         \"model/model-0001.params\", ctx=ctx)\n",
    "    print('use saved model params to start')\n",
    "except:\n",
    "    model = SelfAttentiveBiLSTM(vocab_len, emsize, nhidden, nlayers,\n",
    "                            natt_unit, natt_hops, nfc, nclass,\n",
    "                            drop_prob, pool_way, prune_p, prune_q)\n",
    "\n",
    "    print('initialize a new model')\n",
    "    model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "    model.hybridize()\n",
    "\n",
    "    # Attach a pre-trained glove word vector to the embedding layer\n",
    "    model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "    # fixed the embedding layer\n",
    "    model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.021406  0.399445 -0.150773  0.416859 -0.173093 -0.460412 -0.09578\n",
       " -0.452269 -0.060334  0.178076  0.129666 -0.187627 -0.268714  0.281752\n",
       "  0.672784  0.079613 -0.504229  0.056391 -0.041292 -0.359443  0.09827\n",
       " -0.278529 -0.140741  0.193164  0.061355 -0.310622 -0.198531 -0.223974\n",
       " -0.082908  0.320169 -0.182967 -0.212077  0.134077 -0.236404  0.189204\n",
       "  0.0568    0.298494  0.59866  -0.32057  -0.242243 -0.044432  0.217198\n",
       "  0.202773 -0.211469 -0.173185 -0.098174  0.112375  0.270286 -0.148778\n",
       " -0.42995  -0.418146  0.294171  0.087337  0.670113 -0.030841 -0.420761\n",
       " -0.209422 -0.622883 -0.143712 -0.551885 -0.10698   0.017491  0.247325\n",
       "  0.006137 -0.380092 -0.164557 -0.417308 -0.579596  0.321489  0.255118\n",
       " -0.123747  0.029786  0.354533 -0.066662 -0.078275  0.371021 -0.162618\n",
       " -0.203096 -0.072074 -0.597944  0.238788 -0.080151  0.420655  0.035114\n",
       " -0.431164 -0.094002  0.175306  0.183954 -0.381618  0.202283 -0.009674\n",
       "  0.05437  -0.123119  0.073714 -0.183245  0.02759   0.364248  0.242034\n",
       "  0.409332 -0.225912 -0.009234 -0.498906  0.023972 -0.670852  0.304743\n",
       " -0.110207  0.016727  0.120927  0.359729  0.540678 -0.597677 -0.231297\n",
       " -0.139061 -0.131903 -0.145361 -0.098901 -0.170922  0.166638 -0.068646\n",
       " -0.233326 -0.049805  0.035585  0.162867 -0.097288 -0.633858  0.132744\n",
       "  0.505321 -0.068713 -0.339614  0.274076 -0.81329   0.445863  0.262\n",
       "  0.239334 -0.135528 -0.052305  0.065311  0.254529 -0.24876  -0.063311\n",
       "  0.227849 -0.604022 -0.082248  0.216981 -0.095983 -0.099656  0.502944\n",
       "  0.229551 -0.785893  0.303319  0.006567 -0.274869  0.01777   0.236432\n",
       " -0.208093  0.14204   0.272912 -0.157207  0.408608  0.291401 -0.44746\n",
       "  0.213749 -0.482872 -0.135301  0.086258 -0.128578 -0.314372  0.299549\n",
       " -0.52551  -0.012774  0.30841   0.196687 -0.776894 -0.140688  0.347493\n",
       " -0.093536  0.156901 -0.185869  0.099757 -0.03598   0.407837 -0.202864\n",
       " -0.435425 -0.699509  0.392127  0.200118  0.050621 -0.344119  0.036421\n",
       " -0.195497  0.237631  0.67145   0.332611  0.224635 -0.362532 -0.346812\n",
       " -0.238275 -0.202789  0.388815 -0.01685   0.059274 -0.137705 -0.06066\n",
       " -0.279743  0.195018 -0.172664 -0.07031  -0.482807  0.230374 -0.40433\n",
       " -0.106305  0.318085 -0.034242 -0.056574 -0.464918 -0.362307 -0.054608\n",
       "  0.15312  -0.700729 -0.31564   0.006731  0.095134 -0.034054  0.3824\n",
       "  0.350664 -0.156609 -0.167592  0.200425  0.107554  0.175647  0.174542\n",
       " -0.273973 -0.178119 -0.070842  0.118614 -0.037653  0.442076 -0.102494\n",
       " -0.047897 -0.149223  0.480871 -0.052611  0.059749  0.313609 -0.147787\n",
       "  0.225567 -0.171472  0.220666  0.152394 -0.01921  -0.021683  0.099309\n",
       "  0.162443 -0.149402  0.258414 -0.036681  0.092331 -0.323573  0.240734\n",
       " -0.081331 -0.20972  -0.080878 -0.459424  0.474648 -0.12307   0.112385\n",
       "  0.380406  0.409096  0.478196 -0.406717  0.584524 -0.408327 -0.512668\n",
       " -0.108049  0.290118 -0.131853 -0.117237  0.577378  0.041639  0.30259\n",
       " -0.195129  0.332959 -0.145064 -0.00163  -0.584111 -0.311395  0.082005\n",
       "  0.144737  0.040593 -0.358912  0.193897  0.208329  0.065865 -0.006458\n",
       "  0.035589  0.491693  0.149766  0.393231 -0.072716  0.030412]\n",
       "<NDArray 300 @cpu(0)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding.idx_to_vec[vocab.embedding.token_to_idx['i']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helpers\n",
    "Calculate loss, one epoch computation and top function for train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, y, model, loss, class_weight, penal_coeff):\n",
    "    pred, att = model(x)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name in ['sce', 'l1', 'l2']:\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    # penalty, now we have two att's\n",
    "    diversity_penalty = nd.batch_dot(att, nd.transpose(att, axes=(0, 2, 1))) - \\\n",
    "                        nd.eye(att.shape[1], ctx=att.context)\n",
    "    l = l + penal_coeff * diversity_penalty.norm(axis=(1, 2))\n",
    "\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch,\n",
    "              penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x, batch_y in data_iter:\n",
    "        batch_x = batch_x.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                               loss, class_weight, penal_coeff)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                           loss, class_weight, penal_coeff)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 3 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs,\n",
    "                penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now we will train this model. To handle data inbalance, we first set an estimated weight of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_weight\n",
    "weight_list = get_weight(DATA_FOLDER, LABEL_FILE, 10)\n",
    "class_weight = None\n",
    "loss_name = 'wsce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "penal_coeff = 0.003\n",
    "clip = .5\n",
    "nepochs = 5\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array(weight_list, ctx=ctx)\n",
    "elif loss_name == 'l1':\n",
    "    loss = gluon.loss.L1Loss()\n",
    "elif loss_name == 'l2':\n",
    "    loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 400, batch_train_loss 65.4778, batch_train_acc 0.096\n",
      "epoch 1, batch 800, batch_train_loss 67.1403, batch_train_acc 0.092\n",
      "epoch 1, batch 1200, batch_train_loss 67.1463, batch_train_acc 0.082\n",
      "epoch 1, batch 1600, batch_train_loss 64.2306, batch_train_acc 0.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 66.8441, acc_train 0.102, F1_train 0.050, \n",
      "\t valid_loss 64.8300, acc_valid 0.118, F1_valid 0.055, \n",
      "time 413.33 sec\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, penal_coeff=penal_coeff, \\\n",
    "            clip=clip, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = str(round(time.time()))\n",
    "model.export(\"model/model\"+token, epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATA = 'test.csv'\n",
    "predictions = []\n",
    "test_df = pd.read_csv(DATA_FOLDER+TEST_DATA, header=None, sep='\\t')\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pred len 2000, time 13.13\n",
      "current pred len 4000, time 13.16\n",
      "current pred len 6000, time 13.20\n",
      "current pred len 8000, time 13.00\n",
      "current pred len 10000, time 13.12\n",
      "current pred len 12000, time 13.20\n",
      "current pred len 14000, time 13.19\n",
      "current pred len 16000, time 12.84\n",
      "current pred len 18000, time 13.03\n",
      "current pred len 20000, time 13.09\n",
      "current pred len 22000, time 13.18\n",
      "current pred len 24000, time 13.10\n",
      "current pred len 26000, time 13.20\n",
      "current pred len 28000, time 13.34\n",
      "current pred len 30000, time 13.49\n",
      "current pred len 32000, time 13.19\n",
      "current pred len 34000, time 13.31\n",
      "current pred len 36000, time 13.19\n",
      "current pred len 38000, time 13.18\n",
      "current pred len 40000, time 13.13\n",
      "current pred len 42000, time 13.13\n",
      "current pred len 44000, time 13.06\n",
      "current pred len 46000, time 13.21\n",
      "current pred len 48000, time 13.10\n",
      "current pred len 50000, time 13.06\n",
      "current pred len 52000, time 13.01\n",
      "current pred len 54000, time 13.26\n",
      "current pred len 56000, time 13.22\n",
      "current pred len 58000, time 13.05\n",
      "current pred len 60000, time 13.05\n",
      "current pred len 62000, time 13.24\n",
      "current pred len 64000, time 13.36\n",
      "current pred len 66000, time 13.41\n",
      "current pred len 68000, time 13.28\n",
      "current pred len 70000, time 13.18\n",
      "current pred len 72000, time 13.20\n",
      "current pred len 74000, time 13.17\n",
      "current pred len 76000, time 13.31\n",
      "current pred len 78000, time 12.96\n",
      "current pred len 80000, time 13.07\n",
      "current pred len 82000, time 13.30\n",
      "current pred len 84000, time 13.16\n",
      "current pred len 86000, time 13.23\n",
      "current pred len 88000, time 13.01\n",
      "current pred len 90000, time 13.24\n",
      "current pred len 92000, time 13.31\n",
      "current pred len 94000, time 13.20\n",
      "current pred len 96000, time 13.15\n",
      "current pred len 98000, time 13.15\n",
      "current pred len 100000, time 12.99\n",
      "current pred len 102000, time 12.74\n",
      "current pred len 104000, time 13.45\n",
      "current pred len 106000, time 13.20\n",
      "current pred len 108000, time 13.10\n",
      "current pred len 110000, time 13.27\n",
      "current pred len 112000, time 13.35\n",
      "current pred len 114000, time 12.97\n",
      "current pred len 116000, time 13.26\n",
      "current pred len 118000, time 13.14\n",
      "current pred len 120000, time 13.21\n",
      "current pred len 122000, time 13.39\n",
      "current pred len 124000, time 13.10\n",
      "current pred len 126000, time 12.89\n",
      "current pred len 128000, time 13.11\n",
      "current pred len 130000, time 13.24\n",
      "current pred len 132000, time 13.18\n",
      "current pred len 134000, time 13.24\n",
      "current pred len 136000, time 13.30\n",
      "current pred len 138000, time 13.21\n",
      "current pred len 140000, time 13.35\n",
      "current pred len 142000, time 13.00\n",
      "current pred len 144000, time 13.25\n",
      "current pred len 146000, time 13.34\n",
      "current pred len 148000, time 13.16\n",
      "current pred len 150000, time 13.23\n",
      "current pred len 152000, time 13.14\n",
      "current pred len 154000, time 13.22\n",
      "current pred len 156000, time 13.11\n",
      "current pred len 158000, time 13.23\n",
      "current pred len 160000, time 13.05\n",
      "current pred len 162000, time 13.23\n",
      "current pred len 164000, time 12.98\n",
      "current pred len 166000, time 13.30\n",
      "current pred len 168000, time 13.34\n",
      "current pred len 170000, time 13.33\n",
      "current pred len 172000, time 13.16\n",
      "current pred len 174000, time 13.29\n",
      "current pred len 176000, time 13.39\n",
      "current pred len 178000, time 13.41\n",
      "current pred len 180000, time 13.30\n",
      "current pred len 182000, time 13.23\n",
      "current pred len 184000, time 13.25\n",
      "current pred len 186000, time 13.13\n",
      "current pred len 188000, time 13.03\n",
      "current pred len 190000, time 13.24\n",
      "current pred len 192000, time 13.18\n",
      "current pred len 194000, time 13.09\n",
      "current pred len 196000, time 13.27\n",
      "current pred len 198000, time 13.02\n",
      "current pred len 200000, time 13.22\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for _, tweet in test_df.iterrows():\n",
    "    token = vocab[jieba.lcut(tweet[1])]\n",
    "    inp = nd.array(token, ctx=ctx).reshape(1,-1)\n",
    "    pred, _ = model(inp)\n",
    "    pred = nd.argmax(pred, axis=-1).asscalar()\n",
    "    predictions.append(int(pred))\n",
    "    if len(predictions)%2000==0:\n",
    "        ckpt = time.time()\n",
    "        print('current pred len %d, time %.2fs' % (len(predictions), ckpt-start))\n",
    "        start = ckpt\n",
    "submit = pd.DataFrame({'Expected': predictions})\n",
    "submit.to_csv('submission.csv', sep=',', index_label='ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
