{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import gluonnlp as nlp\n",
    "import jieba\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from d2l import try_gpu\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "WORD_EMBED = 'sgns.weibo.word'\n",
    "LABEL_FILE = 'train.label'\n",
    "N_ROWS=50000\n",
    "ctx = try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_FOLDER+TRAIN_DATA, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690531, 172633)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =[ [row[0], row[1]] for _, row in train_df.iterrows()]\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, .2)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.116 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.173 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.200 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.197 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.263 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.231 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.306 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.483 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.334 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.262 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.361 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.351 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=21.80s, #Sentences=690531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.486 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.530 seconds.\n",
      "Loading model cost 1.525 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.576 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.537 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.548 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.632 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.652 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.812 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.731 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.896 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 2.127 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=7.77s, #Sentences=172633\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(x):\n",
    "    tweet, label = x\n",
    "    word_list = jieba.lcut(tweet)\n",
    "    return word_list, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def to_word_list(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(tokenizer, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_word_list, train_word_lengths = to_word_list(train_dataset)\n",
    "valid_word_list, valid_word_lengths = to_word_list(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/embedding/token_embedding.py:296: UserWarning: line 0 in data/sgns.weibo.word: skipped likely header line.\n",
      "  .format(line_num, pretrained_file_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=40004, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "train_seqs = [sample[0] for sample in train_word_list]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=40000)\n",
    "\n",
    "# load customed pre-trained embedding\n",
    "embedding_weights = nlp.embedding.TokenEmbedding.from_file(file_path=DATA_FOLDER+WORD_EMBED)\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_word_list)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=690531, batch_num=3212\n",
      "  key=[26, 46, 66, 86, 106, 126, 146, 166, 186, 206]\n",
      "  cnt=[591170, 60552, 23618, 11994, 2948, 168, 50, 17, 7, 7]\n",
      "  batch_size=[253, 143, 99, 76, 64, 64, 64, 64, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # in this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_word_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.0000e+00 9.4270e+03 8.9700e+02 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [3.6000e+01 3.1620e+03 6.0000e+01 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [4.9000e+01 4.8000e+01 1.7100e+02 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [7.0000e+00 3.8770e+03 7.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [3.7590e+03 8.9300e+02 3.6000e+01 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [1.0550e+03 1.5282e+04 2.4700e+02 ... 0.0000e+00 0.0000e+00 0.0000e+00]]\n",
      "<NDArray 64x103 @cpu_shared(0)> \n",
      "[9 3 3 9 5 4 2 8 3 1 3 4 6 4 3 5 3 0 9 8 6 1 3 5 5 2 7 9 1 4 1 4 3 3 3 0 0\n",
      " 4 5 5 3 4 9 1 4 0 3 3 6 5 8 4 0 9 3 0 1 3 3 5 4 9 0 5]\n",
      "<NDArray 64 @cpu_shared(0)>\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in train_dataloader:\n",
    "    print(tweet, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model contruction\n",
    "Self attention layer, weighted cross entropy, and whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention(\n",
      "  (ut_dense): Dense(None -> 20, Activation(tanh))\n",
      "  (et_dense): Dense(None -> 5, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# custom attention layer\n",
    "# in this class, we want to implement the operation:\n",
    "# softmax(W_2 * tanh(W_1 * H))\n",
    "# where H is the word embedding of the whole sentence, of shape (num_of_word, embed_size)\n",
    "class SelfAttention(nn.HybridBlock):\n",
    "    def __init__(self, att_unit, att_hops, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # this layer is tanh(w_1 * H), the att_unit corresponds to d_a in the essay\n",
    "            self.ut_dense = nn.Dense(att_unit, activation='tanh', flatten=False)\n",
    "            # this layer implements the multiple hops\n",
    "            self.et_dense = nn.Dense(att_hops, activation=None, flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x): # F is the backend which implements the tensor operation\n",
    "        # x shape: [batch_size, seq_len, embedding_width]\n",
    "        # ut shape: [batch_size, seq_len, att_unit]\n",
    "        ut = self.ut_dense(x) # batch_size * seq_len [* embed_size * embed_size *] att_unit\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)# batch_size * seq_len [* att_unit * att_unit *] att_hops\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        # softmax is performed along the seq_len dimension\n",
    "        att = F.softmax(F.transpose(et, axes=(0, 2, 1)), axis=-1)\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = F.batch_dot(att, x)\n",
    "        # output is the weighted matrix representation of the matrix\n",
    "        # att is the weighted vector we use as attention\n",
    "        return output, att\n",
    "    \n",
    "# d_a = 20, hops = 5\n",
    "print(SelfAttention(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentiveBiLSTM(nn.HybridBlock):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, natt_unit, natt_hops, \\\n",
    "                 nfc, nclass, # these two params are not used currrently\n",
    "                 drop_prob, pool_way, prune_p=None, prune_q=None, **kwargs):\n",
    "        super(SelfAttentiveBiLSTM, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # now we switch back to shared layers\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            \n",
    "            self.bilstm = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, \\\n",
    "                                        bidirectional=True)\n",
    "            \n",
    "            self.att_encoder = SelfAttention(natt_unit, natt_hops)\n",
    "            self.dense = nn.Dense(nfc, activation='tanh')\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.Dense(nclass)\n",
    "            \n",
    "            self.dense_p, self.dense_q = None, None\n",
    "            if all([prune_p, prune_q]):\n",
    "                self.dense_p = nn.Dense(prune_p, activation='tanh', flatten=False)\n",
    "                self.dense_q = nn.Dense(prune_q, activation='tanh', flatten=False)\n",
    "\n",
    "            self.drop_prob = drop_prob\n",
    "            self.pool_way = pool_way\n",
    "\n",
    "    def hybrid_forward(self, F, inp):\n",
    "        # inp_embed size: [batch, seq_len, embed_size]\n",
    "        inp_embed = self.embedding_layer(inp)\n",
    "        # rnn requires the first dimension to be the time steps\n",
    "        h_output = self.bilstm(F.transpose(inp_embed, axes=(1, 0, 2)))\n",
    "        # att_output: [batch, att_hops, emsize]\n",
    "        output, att = self.att_encoder(F.transpose(h_output, axes=(1, 0, 2)))\n",
    "        '''\n",
    "        FIXME: now this code will only work with flatten\n",
    "        '''\n",
    "        dense_input = None\n",
    "        if self.pool_way == 'flatten':\n",
    "            dense_input = F.Dropout(F.flatten(output), self.drop_prob)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        '''\n",
    "        elif self.pool_way == 'mean':\n",
    "            dense_input = F.Dropout(F.mean(att_output, axis=1), self.drop_prob)\n",
    "        elif self.pool_way == 'prune' and all([self.dense_p, self.dense_q]):\n",
    "            # p_section: [batch, att_hops, prune_p]\n",
    "            p_section = self.dense_p(att_output)\n",
    "            # q_section: [batch, emsize, prune_q]\n",
    "            q_section = self.dense_q(F.transpose(att_output, axes=(0, 2, 1)))\n",
    "            dense_input = F.Dropout(F.concat(F.flatten(p_section), F.flatten(q_section), dim=-1), self.drop_prob)\n",
    "        '''\n",
    "        dense_out = self.dense(dense_input)\n",
    "        output = self.output_layer(F.Dropout(dense_out, self.drop_prob))\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use saved model params to start\n",
      "SymbolBlock(\n",
      "\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300   # word embedding size\n",
    "nhidden = 300    # lstm hidden_dim\n",
    "nlayers = 4     # lstm layers\n",
    "natt_unit = 300     # the hidden_units of attention layer\n",
    "natt_hops = 20    # the channels of attention\n",
    "nfc = 512  # last dense layer size\n",
    "nclass = 72 # we have 72 emoji in total\n",
    "\n",
    "drop_prob = 0\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "try:\n",
    "    model = gluon.nn.SymbolBlock.imports(\"model/model-symbol.json\", ['data'], \\\n",
    "                                         \"model/model-0001.params\", ctx=ctx)\n",
    "    print('use saved model params to start')\n",
    "except:\n",
    "    model = SelfAttentiveBiLSTM(vocab_len, emsize, nhidden, nlayers,\n",
    "                            natt_unit, natt_hops, nfc, nclass,\n",
    "                            drop_prob, pool_way, prune_p, prune_q)\n",
    "\n",
    "    print('initialize a new model')\n",
    "    model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "    model.hybridize()\n",
    "\n",
    "    # Attach a pre-trained glove word vector to the embedding layer\n",
    "    model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "    # fixed the embedding layer\n",
    "    model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.021406  0.399445 -0.150773  0.416859 -0.173093 -0.460412 -0.09578\n",
       " -0.452269 -0.060334  0.178076  0.129666 -0.187627 -0.268714  0.281752\n",
       "  0.672784  0.079613 -0.504229  0.056391 -0.041292 -0.359443  0.09827\n",
       " -0.278529 -0.140741  0.193164  0.061355 -0.310622 -0.198531 -0.223974\n",
       " -0.082908  0.320169 -0.182967 -0.212077  0.134077 -0.236404  0.189204\n",
       "  0.0568    0.298494  0.59866  -0.32057  -0.242243 -0.044432  0.217198\n",
       "  0.202773 -0.211469 -0.173185 -0.098174  0.112375  0.270286 -0.148778\n",
       " -0.42995  -0.418146  0.294171  0.087337  0.670113 -0.030841 -0.420761\n",
       " -0.209422 -0.622883 -0.143712 -0.551885 -0.10698   0.017491  0.247325\n",
       "  0.006137 -0.380092 -0.164557 -0.417308 -0.579596  0.321489  0.255118\n",
       " -0.123747  0.029786  0.354533 -0.066662 -0.078275  0.371021 -0.162618\n",
       " -0.203096 -0.072074 -0.597944  0.238788 -0.080151  0.420655  0.035114\n",
       " -0.431164 -0.094002  0.175306  0.183954 -0.381618  0.202283 -0.009674\n",
       "  0.05437  -0.123119  0.073714 -0.183245  0.02759   0.364248  0.242034\n",
       "  0.409332 -0.225912 -0.009234 -0.498906  0.023972 -0.670852  0.304743\n",
       " -0.110207  0.016727  0.120927  0.359729  0.540678 -0.597677 -0.231297\n",
       " -0.139061 -0.131903 -0.145361 -0.098901 -0.170922  0.166638 -0.068646\n",
       " -0.233326 -0.049805  0.035585  0.162867 -0.097288 -0.633858  0.132744\n",
       "  0.505321 -0.068713 -0.339614  0.274076 -0.81329   0.445863  0.262\n",
       "  0.239334 -0.135528 -0.052305  0.065311  0.254529 -0.24876  -0.063311\n",
       "  0.227849 -0.604022 -0.082248  0.216981 -0.095983 -0.099656  0.502944\n",
       "  0.229551 -0.785893  0.303319  0.006567 -0.274869  0.01777   0.236432\n",
       " -0.208093  0.14204   0.272912 -0.157207  0.408608  0.291401 -0.44746\n",
       "  0.213749 -0.482872 -0.135301  0.086258 -0.128578 -0.314372  0.299549\n",
       " -0.52551  -0.012774  0.30841   0.196687 -0.776894 -0.140688  0.347493\n",
       " -0.093536  0.156901 -0.185869  0.099757 -0.03598   0.407837 -0.202864\n",
       " -0.435425 -0.699509  0.392127  0.200118  0.050621 -0.344119  0.036421\n",
       " -0.195497  0.237631  0.67145   0.332611  0.224635 -0.362532 -0.346812\n",
       " -0.238275 -0.202789  0.388815 -0.01685   0.059274 -0.137705 -0.06066\n",
       " -0.279743  0.195018 -0.172664 -0.07031  -0.482807  0.230374 -0.40433\n",
       " -0.106305  0.318085 -0.034242 -0.056574 -0.464918 -0.362307 -0.054608\n",
       "  0.15312  -0.700729 -0.31564   0.006731  0.095134 -0.034054  0.3824\n",
       "  0.350664 -0.156609 -0.167592  0.200425  0.107554  0.175647  0.174542\n",
       " -0.273973 -0.178119 -0.070842  0.118614 -0.037653  0.442076 -0.102494\n",
       " -0.047897 -0.149223  0.480871 -0.052611  0.059749  0.313609 -0.147787\n",
       "  0.225567 -0.171472  0.220666  0.152394 -0.01921  -0.021683  0.099309\n",
       "  0.162443 -0.149402  0.258414 -0.036681  0.092331 -0.323573  0.240734\n",
       " -0.081331 -0.20972  -0.080878 -0.459424  0.474648 -0.12307   0.112385\n",
       "  0.380406  0.409096  0.478196 -0.406717  0.584524 -0.408327 -0.512668\n",
       " -0.108049  0.290118 -0.131853 -0.117237  0.577378  0.041639  0.30259\n",
       " -0.195129  0.332959 -0.145064 -0.00163  -0.584111 -0.311395  0.082005\n",
       "  0.144737  0.040593 -0.358912  0.193897  0.208329  0.065865 -0.006458\n",
       "  0.035589  0.491693  0.149766  0.393231 -0.072716  0.030412]\n",
       "<NDArray 300 @cpu(0)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding.idx_to_vec[vocab.embedding.token_to_idx['i']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helpers\n",
    "Calculate loss, one epoch computation and top function for train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, y, model, loss, class_weight, penal_coeff):\n",
    "    pred, att = model(x)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name in ['sce', 'l1', 'l2']:\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    # penalty, now we have two att's\n",
    "    diversity_penalty = nd.batch_dot(att, nd.transpose(att, axes=(0, 2, 1))) - \\\n",
    "                        nd.eye(att.shape[1], ctx=att.context)\n",
    "    l = l + penal_coeff * diversity_penalty.norm(axis=(1, 2))\n",
    "\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch,\n",
    "              penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x, batch_y in data_iter:\n",
    "        batch_x = batch_x.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                               loss, class_weight, penal_coeff)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                           loss, class_weight, penal_coeff)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 3 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs,\n",
    "                penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now we will train this model. To handle data inbalance, we first set an estimated weight of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_weight\n",
    "weight_list = get_weight(DATA_FOLDER, LABEL_FILE, 10)\n",
    "class_weight = None\n",
    "loss_name = 'wsce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "penal_coeff = 0.003\n",
    "clip = .5\n",
    "nepochs = 10\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array(weight_list, ctx=ctx)\n",
    "elif loss_name == 'l1':\n",
    "    loss = gluon.loss.L1Loss()\n",
    "elif loss_name == 'l2':\n",
    "    loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 400, batch_train_loss 32.3878, batch_train_acc 0.162\n",
      "epoch 1, batch 800, batch_train_loss 26.5626, batch_train_acc 0.118\n",
      "epoch 1, batch 1200, batch_train_loss 30.7981, batch_train_acc 0.154\n",
      "epoch 1, batch 1600, batch_train_loss 31.4412, batch_train_acc 0.213\n",
      "epoch 1, batch 2000, batch_train_loss 28.8166, batch_train_acc 0.206\n",
      "epoch 1, batch 2400, batch_train_loss 30.9995, batch_train_acc 0.162\n",
      "epoch 1, batch 2800, batch_train_loss 31.1890, batch_train_acc 0.182\n",
      "epoch 1, batch 3200, batch_train_loss 29.1437, batch_train_acc 0.221\n",
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 29.9985, acc_train 0.203, F1_train 0.150, \n",
      "\t valid_loss 30.6038, acc_valid 0.185, F1_valid 0.113, \n",
      "time 636.97 sec\n",
      "****************************************************************************************************\n",
      "epoch 2, batch 400, batch_train_loss 31.6179, batch_train_acc 0.182\n",
      "epoch 2, batch 800, batch_train_loss 29.4696, batch_train_acc 0.294\n",
      "epoch 2, batch 1200, batch_train_loss 28.7025, batch_train_acc 0.190\n",
      "epoch 2, batch 1600, batch_train_loss 30.4275, batch_train_acc 0.225\n",
      "epoch 2, batch 2000, batch_train_loss 29.5701, batch_train_acc 0.182\n",
      "epoch 2, batch 2400, batch_train_loss 29.6873, batch_train_acc 0.174\n",
      "epoch 2, batch 2800, batch_train_loss 30.2054, batch_train_acc 0.194\n",
      "epoch 2, batch 3200, batch_train_loss 29.1679, batch_train_acc 0.245\n",
      "epoch 2, learning_rate 0.00100 \n",
      "\t train_loss 28.8489, acc_train 0.227, F1_train 0.180, \n",
      "\t valid_loss 30.5369, acc_valid 0.190, F1_valid 0.126, \n",
      "time 649.55 sec\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, penal_coeff=penal_coeff, \\\n",
    "            clip=clip, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(\"model/model\", epoch=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
