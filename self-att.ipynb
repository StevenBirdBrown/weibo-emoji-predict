{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user_data/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import gluonnlp as nlp\n",
    "import pkuseg\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from d2l import try_gpu\n",
    "import itertools\n",
    "import jieba\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "WORD_EMBED = 'sgns.weibo.word'\n",
    "LABEL_FILE = 'train.label'\n",
    "N_ROWS=10000\n",
    "ctx = mx.gpu(1)\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_FOLDER+TRAIN_DATA, sep='|')\n",
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776847, 86317)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =[ [row[0], row[1]] for _, row in train_df.iterrows()]\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, .1)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=62.54s, #Sentences=776847\n",
      "Done! Tokenizing Time=7.20s, #Sentences=86317\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(x):\n",
    "    tweet, label = x\n",
    "    if type(tweet) != str:\n",
    "        tweet = str(tweet)\n",
    "    word_list = seg.cut(tweet)\n",
    "    return word_list, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def to_word_list(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(tokenizer, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_word_list, train_word_lengths = to_word_list(train_dataset)\n",
    "valid_word_list, valid_word_lengths = to_word_list(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wup/.local/lib/python3.6/site-packages/gluonnlp/embedding/token_embedding.py:296: UserWarning: line 0 in data/sgns.weibo.bigram-char: skipped likely header line.\n",
      "  .format(line_num, pretrained_file_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=200004, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "train_seqs = [sample[0] for sample in train_word_list]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=200000)\n",
    "\n",
    "# load customed pre-trained embedding\n",
    "embedding_weights = nlp.embedding.TokenEmbedding.from_file(file_path=DATA_FOLDER+WORD_EMBED)\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_word_list)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=776847, batch_num=4617\n",
      "  key=[22, 35, 48, 61, 74, 87, 100, 113, 126, 139]\n",
      "  cnt=[637662, 69980, 31150, 17042, 11028, 7082, 2748, 149, 3, 3]\n",
      "  batch_size=[202, 127, 92, 72, 64, 64, 64, 64, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # in this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_word_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [4.9000e+02 5.0000e+00 1.4615e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [1.5000e+01 9.2900e+02 5.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [6.3300e+02 5.1000e+01 3.2100e+02 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [3.2600e+02 1.6300e+03 1.1996e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [1.0500e+02 4.2000e+01 1.4000e+01 ... 0.0000e+00 0.0000e+00 0.0000e+00]]\n",
      "<NDArray 202x22 @cpu_shared(0)> \n",
      "[34 18 10 15  3 29 31 30 17  3 14  1 34 42 27 28 27 34 14 29 34 34 23 14\n",
      " 31 12 34 35 13 29 31 12 40 19 42 29 16  3 50 34 70  6 19 14 27 35 17 12\n",
      " 35 60 15 41 39 12 61 58 70 22 31 23  9 23 24  6 14 20 12  1 29 40 15 43\n",
      " 55 15 31  3  1  3 29 17  3 37 14 16 29 13 12 47 30  3  3 62 36 33 35 10\n",
      " 52 29 12  1 14 33  3  3  3 12 16 15 14 16 29 15 31 60  3 31 59  3 19 34\n",
      " 53 27 35 23 19 35 40 35 25 52 34 27 70 60 27  9 70 52 10 35 18 34 12 22\n",
      " 31 34 50  3 16  3 17 14 32 34 31 52 34  3 29 38  3  3  3 28 24 70 14 14\n",
      " 37 29 35 11 40 31 14 29 12 68 52 15  3 35 52 29 50 22 21 28 58 23 14 15\n",
      " 28 35 54 24 23 34 34 12 18  3]\n",
      "<NDArray 202 @cpu_shared(0)>\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in train_dataloader:\n",
    "    print(tweet, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model contruction\n",
    "Self attention layer, weighted cross entropy, and whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention(\n",
      "  (ut_dense): Dense(None -> 20, Activation(relu))\n",
      "  (et_dense): Dense(None -> 5, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# custom attention layer\n",
    "# in this class, we want to implement the operation:\n",
    "# softmax(W_2 * tanh(W_1 * H))\n",
    "# where H is the word embedding of the whole sentence, of shape (num_of_word, embed_size)\n",
    "class SelfAttention(nn.HybridBlock):\n",
    "    def __init__(self, att_unit, att_hops, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # this layer is tanh(w_1 * H), the att_unit corresponds to d_a in the essay\n",
    "            self.ut_dense = nn.Dense(att_unit, activation='relu', flatten=False)\n",
    "            # this layer implements the multiple hops\n",
    "            self.et_dense = nn.Dense(att_hops, activation=None, flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x): # F is the backend which implements the tensor operation\n",
    "        # x shape: [batch_size, seq_len, embedding_width]\n",
    "        # ut shape: [batch_size, seq_len, att_unit]\n",
    "        ut = self.ut_dense(x) # batch_size * seq_len [* embed_size * embed_size *] att_unit\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)# batch_size * seq_len [* att_unit * att_unit *] att_hops\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        # softmax is performed along the seq_len dimension\n",
    "        att = F.softmax(F.transpose(et, axes=(0, 2, 1)), axis=-1)\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = F.batch_dot(att, x)\n",
    "        # output is the weighted matrix representation of the matrix\n",
    "        # att is the weighted vector we use as attention\n",
    "        return output, att\n",
    "    \n",
    "# d_a = 20, hops = 5\n",
    "print(SelfAttention(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentiveBiLSTM(nn.HybridBlock):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, natt_unit, natt_hops, \\\n",
    "                 nfc, nclass, # these two params are not used currrently\n",
    "                 drop_prob, pool_way, prune_p=None, prune_q=None, **kwargs):\n",
    "        super(SelfAttentiveBiLSTM, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # now we switch back to shared layers\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            \n",
    "            self.bilstm = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, \\\n",
    "                                   bidirectional=True)\n",
    "            \n",
    "            self.att_encoder = SelfAttention(natt_unit, natt_hops)\n",
    "            self.dense = nn.Dense(nfc, activation='relu')\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.Dense(nclass)\n",
    "            \n",
    "            self.dense_p, self.dense_q = None, None\n",
    "            if all([prune_p, prune_q]):\n",
    "                self.dense_p = nn.Dense(prune_p, activation='relu', flatten=False)\n",
    "                self.dense_q = nn.Dense(prune_q, activation='relu', flatten=False)\n",
    "\n",
    "            self.drop_prob = drop_prob\n",
    "            self.pool_way = pool_way\n",
    "\n",
    "    def hybrid_forward(self, F, inp):\n",
    "        # inp_embed size: [batch, seq_len, embed_size]\n",
    "        inp_embed = self.embedding_layer(inp)\n",
    "        # rnn requires the first dimension to be the time steps\n",
    "        h_output = self.bilstm(F.transpose(inp_embed, axes=(1, 0, 2)))\n",
    "        # att_output: [batch, att_hops, emsize]\n",
    "        output, att = self.att_encoder(F.transpose(h_output, axes=(1, 0, 2)))\n",
    "        '''\n",
    "        FIXME: now this code will only work with flatten\n",
    "        '''\n",
    "        dense_input = None\n",
    "        if self.pool_way == 'flatten':\n",
    "            dense_input = F.Dropout(F.flatten(output), self.drop_prob)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        '''\n",
    "        elif self.pool_way == 'mean':\n",
    "            dense_input = F.Dropout(F.mean(att_output, axis=1), self.drop_prob)\n",
    "        elif self.pool_way == 'prune' and all([self.dense_p, self.dense_q]):\n",
    "            # p_section: [batch, att_hops, prune_p]\n",
    "            p_section = self.dense_p(att_output)\n",
    "            # q_section: [batch, emsize, prune_q]\n",
    "            q_section = self.dense_q(F.transpose(att_output, axes=(0, 2, 1)))\n",
    "            dense_input = F.Dropout(F.concat(F.flatten(p_section), F.flatten(q_section), dim=-1), self.drop_prob)\n",
    "        '''\n",
    "        dense_out = self.dense(dense_input)\n",
    "        output = self.output_layer(F.Dropout(dense_out, self.drop_prob))\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize a new model\n",
      "gpu(1) SelfAttentiveBiLSTM(\n",
      "  (embedding_layer): Embedding(200004 -> 300, float32)\n",
      "  (bilstm): GRU(None -> 200, TNC, bidirectional)\n",
      "  (att_encoder): SelfAttention(\n",
      "    (ut_dense): Dense(None -> 200, Activation(relu))\n",
      "    (et_dense): Dense(None -> 10, linear)\n",
      "  )\n",
      "  (dense): Dense(None -> 128, Activation(relu))\n",
      "  (output_layer): Dense(None -> 72, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300   # word embedding size\n",
    "nhidden = 200    # lstm hidden_dim\n",
    "nlayers = 1     # lstm layers\n",
    "natt_unit = 200     # the hidden_units of attention layer\n",
    "natt_hops = 10    # the channels of attention\n",
    "nfc = 128  # last dense layer size\n",
    "nclass = 72 # we have 72 emoji in total\n",
    "\n",
    "drop_prob = 0.\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "\n",
    "try:\n",
    "    assert(False)\n",
    "    model = gluon.nn.SymbolBlock.imports(\"model/model-symbol.json\", ['data'], \\\n",
    "                                         \"model/model-0001.params\", ctx=ctx)\n",
    "    print('use saved model params to start')\n",
    "except:\n",
    "    model = SelfAttentiveBiLSTM(vocab_len, emsize, nhidden, nlayers,\n",
    "                            natt_unit, natt_hops, nfc, nclass,\n",
    "                            drop_prob, pool_way, prune_p, prune_q)\n",
    "\n",
    "    print('initialize a new model')\n",
    "    model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "    model.hybridize()\n",
    "\n",
    "    # Attach a pre-trained glove word vector to the embedding layer\n",
    "    model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "    # fixed the embedding layer\n",
    "    model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(ctx, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-0.216152  0.111755  0.131167  0.108303 -0.56618   0.154915 -0.682917\n",
       "  0.184372 -1.259019 -0.270754 -0.295431 -0.477648  0.313637 -0.249563\n",
       "  0.464083 -0.177201  0.250866  0.63421  -0.023141 -0.083413 -0.086886\n",
       "  0.373867 -0.120577  0.154108  0.075599  0.749676  0.064633  0.346573\n",
       " -0.375857  0.170967 -0.387877  0.621435  0.252638 -0.327384  0.03449\n",
       "  0.59719   0.396766  0.550666 -0.658407 -0.525238  0.167532 -0.511287\n",
       " -0.360124 -0.815612 -0.511149 -0.866398  0.068793 -0.629899  0.036555\n",
       " -0.245903 -0.501821  0.222177 -0.887551 -0.059061  0.357666  0.444045\n",
       " -0.632446  0.706885  0.488229  0.459782  0.109316 -0.090775 -0.408769\n",
       "  0.25539   0.630114 -0.136657 -0.541437  0.510262 -0.273591  0.137092\n",
       " -0.586211 -0.199848  0.066356  0.603941 -0.288794 -0.023497 -0.258354\n",
       "  0.341849  0.427584 -0.001543  0.755926  0.718712 -1.017008  0.452808\n",
       "  0.173271  0.29188   0.644698  0.49299   0.216398  0.517657  0.00933\n",
       "  0.858918 -0.384057 -0.178975 -0.281533  1.395328  0.856061  0.560499\n",
       " -0.335769 -0.071375 -0.201207 -0.677237  0.587448  0.076399  1.100911\n",
       " -0.912248 -0.57333  -0.50441  -0.391133 -0.12867  -0.274657  0.448127\n",
       " -0.575652  0.33706  -0.149754  0.090467 -0.492194 -0.081431 -0.260503\n",
       "  0.111296  0.205207  0.260449  0.441846  0.679927  0.094967 -0.077246\n",
       " -0.478524  0.140242  0.270564 -0.381547 -0.240794  0.595419 -0.336331\n",
       " -0.991825  0.205368 -0.095051 -0.681293  0.986179  0.585376  0.474926\n",
       " -0.319181 -0.125708 -0.035296  1.298804  0.364361 -0.959611 -0.005816\n",
       " -0.257128  0.321672  0.24452  -0.057346  0.233022 -0.231019  0.689146\n",
       " -0.885067 -0.769348 -0.402978  0.006292 -0.008371  0.58447   0.214406\n",
       "  0.07519   0.26402   0.263128 -0.352657  0.107167 -0.754611  0.144382\n",
       " -0.223209 -0.001827 -0.413423  1.432451 -0.194973  0.434085  0.0993\n",
       "  0.546444 -0.587421  0.711087 -0.420685  0.111468 -0.449062  0.576244\n",
       "  0.176635 -0.728521 -0.182665 -0.338811  0.380492 -0.866309  0.022187\n",
       "  0.392927 -0.467832 -0.053406 -0.574791 -0.847851 -0.28072  -0.367377\n",
       " -0.210802  0.279111  0.67106   0.398311  0.401414  0.035663  0.361546\n",
       " -0.248559 -0.209737 -0.112492 -0.281607  0.365745  0.656992  0.79984\n",
       "  0.562788 -0.008432  0.088128 -0.539908 -1.034308 -0.213033  0.584708\n",
       "  0.097     0.192504  0.138019  0.021104 -0.273847 -0.396205  0.251443\n",
       " -0.233656  0.237069  0.284046 -0.522521 -0.304831 -0.2889    0.138421\n",
       "  0.671724 -0.382596  0.307982  0.282629  0.121924  0.37085   0.296891\n",
       " -0.321504  0.544077 -0.536023  0.238942  0.281211 -0.226763 -0.076582\n",
       "  0.009918 -0.582249  0.444073  0.252879 -0.341321  0.325932  0.583666\n",
       " -0.089971 -0.562199  0.217983  0.917098 -0.183431 -0.437333 -0.760043\n",
       "  0.129498  0.032326 -0.173446 -0.750573  0.273462  0.309773 -0.474446\n",
       "  0.173527 -0.082014  0.429216 -0.298323  0.136077 -0.428049  0.393772\n",
       "  0.23411  -0.313742  0.292258 -0.15159   0.098632 -0.177718 -0.301809\n",
       "  0.383317 -0.400927  0.021241 -0.390365  0.165357 -0.215971 -0.31047\n",
       " -0.099302 -0.629251  0.143659  0.10045  -0.031504  0.359497 -1.069457\n",
       "  0.024579  0.060824 -0.446689  0.821554  0.157456 -0.163165]\n",
       "<NDArray 300 @cpu(0)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding.idx_to_vec[vocab.embedding.token_to_idx['i']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helpers\n",
    "Calculate loss, one epoch computation and top function for train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, y, model, loss, class_weight, penal_coeff):\n",
    "    pred, att = model(x)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name in ['sce', 'l1', 'l2']:\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    # penalty, now we have two att's\n",
    "    diversity_penalty = nd.batch_dot(att, nd.transpose(att, axes=(0, 2, 1))) - \\\n",
    "                        nd.eye(att.shape[1], ctx=att.context)\n",
    "    l = l + penal_coeff * diversity_penalty.norm(axis=(1, 2))\n",
    "\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch,\n",
    "              penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x, batch_y in data_iter:\n",
    "        batch_x = batch_x.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                               loss, class_weight, penal_coeff)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                           loss, class_weight, penal_coeff)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 3 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs,\n",
    "                penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now we will train this model. To handle data inbalance, we first set an estimated weight of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_weight\n",
    "weight_list = get_weight(DATA_FOLDER, LABEL_FILE)\n",
    "# print(weight_list)\n",
    "class_weight = None\n",
    "loss_name = 'sce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "penal_coeff = 0.0003\n",
    "clip = .5\n",
    "nepochs = 5\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array(weight_list, ctx=ctx)\n",
    "elif loss_name == 'l1':\n",
    "    loss = gluon.loss.L1Loss()\n",
    "elif loss_name == 'l2':\n",
    "    loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu(1)\n",
      "epoch 1, batch 400, batch_train_loss 3.5288, batch_train_acc 0.181\n",
      "epoch 1, batch 800, batch_train_loss 3.4437, batch_train_acc 0.213\n",
      "epoch 1, batch 1200, batch_train_loss 3.4180, batch_train_acc 0.193\n",
      "epoch 1, batch 1600, batch_train_loss 3.3982, batch_train_acc 0.158\n",
      "epoch 1, batch 2000, batch_train_loss 3.4827, batch_train_acc 0.129\n",
      "epoch 1, batch 2400, batch_train_loss 3.5532, batch_train_acc 0.158\n",
      "epoch 1, batch 2800, batch_train_loss 3.4652, batch_train_acc 0.163\n",
      "epoch 1, batch 3200, batch_train_loss 3.4479, batch_train_acc 0.124\n",
      "epoch 1, batch 3600, batch_train_loss 3.4505, batch_train_acc 0.158\n",
      "epoch 1, batch 4000, batch_train_loss 3.2319, batch_train_acc 0.213\n",
      "epoch 1, batch 4400, batch_train_loss 3.1403, batch_train_acc 0.196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wup/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 3.4646, acc_train 0.162, F1_train 0.047, \n",
      "\t valid_loss 3.4134, acc_valid 0.169, F1_valid 0.051, \n",
      "time 96.43 sec\n",
      "****************************************************************************************************\n",
      "epoch 2, batch 400, batch_train_loss 3.2242, batch_train_acc 0.168\n",
      "epoch 2, batch 800, batch_train_loss 3.3861, batch_train_acc 0.196\n",
      "epoch 2, batch 1200, batch_train_loss 3.2675, batch_train_acc 0.183\n",
      "epoch 2, batch 1600, batch_train_loss 3.0853, batch_train_acc 0.217\n",
      "epoch 2, batch 2000, batch_train_loss 3.3806, batch_train_acc 0.217\n",
      "epoch 2, batch 2400, batch_train_loss 3.4099, batch_train_acc 0.153\n",
      "epoch 2, batch 2800, batch_train_loss 3.4599, batch_train_acc 0.158\n",
      "epoch 2, batch 3200, batch_train_loss 3.4904, batch_train_acc 0.173\n",
      "epoch 2, batch 3600, batch_train_loss 3.2755, batch_train_acc 0.205\n",
      "epoch 2, batch 4000, batch_train_loss 3.3430, batch_train_acc 0.203\n",
      "epoch 2, batch 4400, batch_train_loss 3.3066, batch_train_acc 0.198\n",
      "epoch 2, learning_rate 0.00100 \n",
      "\t train_loss 3.3447, acc_train 0.178, F1_train 0.063, \n",
      "\t valid_loss 3.3584, acc_valid 0.176, F1_valid 0.059, \n",
      "time 97.40 sec\n",
      "****************************************************************************************************\n",
      "epoch 3, batch 400, batch_train_loss 3.4607, batch_train_acc 0.109\n",
      "epoch 3, batch 800, batch_train_loss 3.3625, batch_train_acc 0.188\n",
      "epoch 3, batch 1200, batch_train_loss 3.2136, batch_train_acc 0.203\n",
      "epoch 3, batch 1600, batch_train_loss 3.2556, batch_train_acc 0.193\n",
      "epoch 3, batch 2000, batch_train_loss 3.1332, batch_train_acc 0.236\n",
      "epoch 3, batch 2400, batch_train_loss 3.1711, batch_train_acc 0.243\n",
      "epoch 3, batch 2800, batch_train_loss 3.1747, batch_train_acc 0.205\n",
      "epoch 3, batch 3200, batch_train_loss 3.2948, batch_train_acc 0.233\n",
      "epoch 3, batch 3600, batch_train_loss 3.2427, batch_train_acc 0.218\n",
      "epoch 3, batch 4000, batch_train_loss 3.3169, batch_train_acc 0.183\n",
      "epoch 3, batch 4400, batch_train_loss 3.2330, batch_train_acc 0.188\n",
      "epoch 3, learning_rate 0.00100 \n",
      "\t train_loss 3.2872, acc_train 0.185, F1_train 0.070, \n",
      "\t valid_loss 3.3544, acc_valid 0.178, F1_valid 0.058, \n",
      "time 93.53 sec\n",
      "****************************************************************************************************\n",
      "epoch 4, batch 400, batch_train_loss 3.1034, batch_train_acc 0.228\n",
      "epoch 4, batch 800, batch_train_loss 3.1409, batch_train_acc 0.223\n",
      "epoch 4, batch 1200, batch_train_loss 3.2328, batch_train_acc 0.208\n",
      "epoch 4, batch 1600, batch_train_loss 3.1349, batch_train_acc 0.219\n",
      "epoch 4, batch 2000, batch_train_loss 3.4436, batch_train_acc 0.144\n",
      "epoch 4, batch 2400, batch_train_loss 3.3091, batch_train_acc 0.198\n",
      "epoch 4, batch 2800, batch_train_loss 3.2437, batch_train_acc 0.153\n",
      "epoch 4, batch 3200, batch_train_loss 3.3327, batch_train_acc 0.198\n",
      "epoch 4, batch 3600, batch_train_loss 2.9657, batch_train_acc 0.283\n",
      "epoch 4, batch 4000, batch_train_loss 3.1393, batch_train_acc 0.233\n",
      "epoch 4, batch 4400, batch_train_loss 3.2720, batch_train_acc 0.208\n",
      "epoch 4, learning_rate 0.00090 \n",
      "\t train_loss 3.2275, acc_train 0.192, F1_train 0.078, \n",
      "\t valid_loss 3.3429, acc_valid 0.181, F1_valid 0.062, \n",
      "time 97.77 sec\n",
      "****************************************************************************************************\n",
      "epoch 5, batch 400, batch_train_loss 3.2045, batch_train_acc 0.220\n",
      "epoch 5, batch 800, batch_train_loss 3.1264, batch_train_acc 0.244\n",
      "epoch 5, batch 1200, batch_train_loss 3.3799, batch_train_acc 0.144\n",
      "epoch 5, batch 1600, batch_train_loss 3.3119, batch_train_acc 0.168\n",
      "epoch 5, batch 2000, batch_train_loss 2.5351, batch_train_acc 0.306\n",
      "epoch 5, batch 2400, batch_train_loss 3.3409, batch_train_acc 0.178\n",
      "epoch 5, batch 2800, batch_train_loss 3.2487, batch_train_acc 0.203\n",
      "epoch 5, batch 3200, batch_train_loss 2.6364, batch_train_acc 0.344\n",
      "epoch 5, batch 3600, batch_train_loss 3.3468, batch_train_acc 0.168\n",
      "epoch 5, batch 4000, batch_train_loss 3.2415, batch_train_acc 0.183\n",
      "epoch 5, batch 4400, batch_train_loss 2.8090, batch_train_acc 0.250\n",
      "epoch 5, learning_rate 0.00090 \n",
      "\t train_loss 3.1691, acc_train 0.199, F1_train 0.088, \n",
      "\t valid_loss 3.3460, acc_valid 0.179, F1_valid 0.064, \n",
      "time 98.65 sec\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "print(ctx)\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, penal_coeff=penal_coeff, \\\n",
    "            clip=clip, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2f8bd6fef94c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model/self-att-179\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.export(\"model/self-att-179\", epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_DATA = 'test.csv'\n",
    "test_df = pd.read_csv(DATA_FOLDER+TEST_DATA, header=None, sep='\\t')\n",
    "\n",
    "model = gluon.nn.SymbolBlock.imports(\"model/self-att-207-symbol.json\", ['data'], \\\n",
    "                                     \"model/self-att-207-0001.params\", ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pred len 2000, time 9.44s\n",
      "current pred len 4000, time 9.23s\n",
      "current pred len 6000, time 9.37s\n",
      "current pred len 8000, time 9.25s\n",
      "current pred len 10000, time 9.29s\n",
      "current pred len 12000, time 7.66s\n",
      "current pred len 14000, time 6.21s\n",
      "current pred len 16000, time 8.77s\n",
      "current pred len 18000, time 9.16s\n",
      "current pred len 20000, time 8.67s\n",
      "current pred len 22000, time 9.03s\n",
      "current pred len 24000, time 9.26s\n",
      "current pred len 26000, time 9.28s\n",
      "current pred len 28000, time 8.79s\n",
      "current pred len 30000, time 9.34s\n",
      "current pred len 32000, time 9.34s\n",
      "current pred len 34000, time 9.42s\n",
      "current pred len 36000, time 9.38s\n",
      "current pred len 38000, time 9.30s\n",
      "current pred len 40000, time 9.38s\n",
      "current pred len 42000, time 9.17s\n",
      "current pred len 44000, time 9.31s\n",
      "current pred len 46000, time 9.31s\n",
      "current pred len 48000, time 9.22s\n",
      "current pred len 50000, time 9.25s\n",
      "current pred len 52000, time 9.16s\n",
      "current pred len 54000, time 9.40s\n",
      "current pred len 56000, time 9.21s\n",
      "current pred len 58000, time 8.86s\n",
      "current pred len 60000, time 8.81s\n",
      "current pred len 62000, time 9.12s\n",
      "current pred len 64000, time 9.24s\n",
      "current pred len 66000, time 8.56s\n",
      "current pred len 68000, time 9.29s\n",
      "current pred len 70000, time 9.30s\n",
      "current pred len 72000, time 9.22s\n",
      "current pred len 74000, time 8.88s\n",
      "current pred len 76000, time 9.17s\n",
      "current pred len 78000, time 8.95s\n",
      "current pred len 80000, time 9.21s\n",
      "current pred len 82000, time 9.20s\n",
      "current pred len 84000, time 9.33s\n",
      "current pred len 86000, time 8.98s\n",
      "current pred len 88000, time 9.14s\n",
      "current pred len 90000, time 9.13s\n",
      "current pred len 92000, time 9.26s\n",
      "current pred len 94000, time 9.32s\n",
      "current pred len 96000, time 8.87s\n",
      "current pred len 98000, time 8.51s\n",
      "current pred len 100000, time 9.08s\n",
      "current pred len 102000, time 8.92s\n",
      "current pred len 104000, time 9.32s\n",
      "current pred len 106000, time 8.53s\n",
      "current pred len 108000, time 9.39s\n",
      "current pred len 110000, time 9.36s\n",
      "current pred len 112000, time 9.32s\n",
      "current pred len 114000, time 9.14s\n",
      "current pred len 116000, time 9.14s\n",
      "current pred len 118000, time 8.82s\n",
      "current pred len 120000, time 9.25s\n",
      "current pred len 122000, time 9.14s\n",
      "current pred len 124000, time 9.33s\n",
      "current pred len 126000, time 9.30s\n",
      "current pred len 128000, time 9.21s\n",
      "current pred len 130000, time 9.42s\n",
      "current pred len 132000, time 9.43s\n",
      "current pred len 134000, time 9.37s\n",
      "current pred len 136000, time 9.41s\n",
      "current pred len 138000, time 6.44s\n",
      "current pred len 140000, time 8.26s\n",
      "current pred len 142000, time 9.26s\n",
      "current pred len 144000, time 9.37s\n",
      "current pred len 146000, time 8.73s\n",
      "current pred len 148000, time 9.11s\n",
      "current pred len 150000, time 9.42s\n",
      "current pred len 152000, time 9.42s\n",
      "current pred len 154000, time 9.46s\n",
      "current pred len 156000, time 9.31s\n",
      "current pred len 158000, time 9.43s\n",
      "current pred len 160000, time 9.11s\n",
      "current pred len 162000, time 9.23s\n",
      "current pred len 164000, time 9.29s\n",
      "current pred len 166000, time 9.38s\n",
      "current pred len 168000, time 9.31s\n",
      "current pred len 170000, time 9.47s\n",
      "current pred len 172000, time 9.43s\n",
      "current pred len 174000, time 9.56s\n",
      "current pred len 176000, time 9.25s\n",
      "current pred len 178000, time 8.40s\n",
      "current pred len 180000, time 9.38s\n",
      "current pred len 182000, time 9.25s\n",
      "current pred len 184000, time 8.98s\n",
      "current pred len 186000, time 8.84s\n",
      "current pred len 188000, time 9.17s\n",
      "current pred len 190000, time 9.24s\n",
      "current pred len 192000, time 9.32s\n",
      "current pred len 194000, time 9.33s\n",
      "current pred len 196000, time 9.47s\n",
      "current pred len 198000, time 9.27s\n",
      "current pred len 200000, time 9.23s\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "start = time.time()\n",
    "for _, tweet in test_df.iterrows():\n",
    "    token = vocab[seg.cut(tweet[1])]\n",
    "    if len(token)==0:\n",
    "        token = [0.]\n",
    "    inp = nd.array(token, ctx=ctx).reshape(1,-1)\n",
    "    pred, _ = model(inp)\n",
    "    pred = nd.argmax(pred, axis=1).asscalar()\n",
    "    predictions.append(int(pred))\n",
    "    if len(predictions)%2000==0:\n",
    "        ckpt = time.time()\n",
    "        print('current pred len %d, time %.2fs' % (len(predictions), ckpt-start))\n",
    "        start = ckpt\n",
    "submit = pd.DataFrame({'Expected': predictions})\n",
    "submit.to_csv('submission.csv', sep=',', index_label='ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
