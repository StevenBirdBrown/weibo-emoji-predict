{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import gluonnlp as nlp\n",
    "import pkuseg\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from d2l import try_gpu\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "WORD_EMBED = 'sgns.weibo.bigram-char'\n",
    "LABEL_FILE = 'train.label'\n",
    "N_ROWS=10000\n",
    "ctx = try_gpu()\n",
    "seg = pkuseg.pkuseg(model_name='web')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_FOLDER+TRAIN_DATA, nrows=N_ROWS, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =[ [row[0], row[1]] for _, row in train_df.iterrows()]\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, .2)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=1.01s, #Sentences=8000\n",
      "Done! Tokenizing Time=0.47s, #Sentences=2000\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(x):\n",
    "    tweet, label = x\n",
    "    if type(tweet) != str:\n",
    "        tweet = str(tweet)\n",
    "    word_list = seg.cut(tweet)\n",
    "    return word_list, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def to_word_list(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(tokenizer, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_word_list, train_word_lengths = to_word_list(train_dataset)\n",
    "valid_word_list, valid_word_lengths = to_word_list(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/embedding/token_embedding.py:296: UserWarning: line 0 in data/sgns.weibo.bigram-char: skipped likely header line.\n",
      "  .format(line_num, pretrained_file_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=17473, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "train_seqs = [sample[0] for sample in train_word_list]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=40000)\n",
    "\n",
    "# load customed pre-trained embedding\n",
    "embedding_weights = nlp.embedding.TokenEmbedding.from_file(file_path=DATA_FOLDER+WORD_EMBED)\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_word_list)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=8000, batch_num=55\n",
      "  key=[14, 23, 32, 41, 50, 59, 68, 77, 86, 95]\n",
      "  cnt=[5902, 942, 449, 255, 140, 110, 97, 57, 29, 19]\n",
      "  batch_size=[217, 132, 95, 74, 64, 64, 64, 64, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # in this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_word_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[1.100e+02 5.000e+01 1.020e+02 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [3.140e+03 5.000e+00 3.594e+03 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [3.760e+02 3.122e+03 4.500e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [2.389e+03 1.070e+02 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.600e+01 5.475e+03 4.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [1.634e+03 2.100e+01 2.800e+01 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "<NDArray 217x14 @cpu_shared(0)> \n",
      "[ 3 30 39 10 47 23 26  0  3  0 68 29 42 38 35 34  3 31 12 16 29 20 24  3\n",
      " 18 29 35 23  6 17 23 29  9 39  3 34 49  3 61 12 16 12 30 28 43 40  8 47\n",
      "  6 29 12 30  3 29  0 64 67  9 16 58 23  0 23  3  1 12  8 35 43 16  1 16\n",
      "  0  3  3 12 38 22  3 52 32 41 29 42 12 23  6 43 17  6 23  6  6  3 16  3\n",
      " 14 31 11 24 23 36 12 28 41 35 17 55 12  3 36 29 14 35  3 30 23 41 31 14\n",
      " 12 12 16 30 19 33 23 35 27 14 29 23 24  7 34 14 34  1 34  3 11 12 14 23\n",
      "  6 14 23 23  0 52 23 12 30 10  0 12 10 14 16 16 34 12 31  6 29 12 62 13\n",
      " 35 62 35 23 24 17  3 14  4 14  9  6  3 18 47 23 12 48 12 12 12 48 35  5\n",
      " 13 24 30  3  0 31 23  6 34 62 31 52 31 28 12 37  3  3  1 52 14 11 32 18\n",
      " 12]\n",
      "<NDArray 217 @cpu_shared(0)>\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in train_dataloader:\n",
    "    print(tweet, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model contruction\n",
    "Self attention layer, weighted cross entropy, and whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention(\n",
      "  (ut_dense): Dense(None -> 20, Activation(tanh))\n",
      "  (et_dense): Dense(None -> 5, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# custom attention layer\n",
    "# in this class, we want to implement the operation:\n",
    "# softmax(W_2 * tanh(W_1 * H))\n",
    "# where H is the word embedding of the whole sentence, of shape (num_of_word, embed_size)\n",
    "class SelfAttention(nn.HybridBlock):\n",
    "    def __init__(self, att_unit, att_hops, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # this layer is tanh(w_1 * H), the att_unit corresponds to d_a in the essay\n",
    "            self.ut_dense = nn.Dense(att_unit, activation='tanh', flatten=False)\n",
    "            # this layer implements the multiple hops\n",
    "            self.et_dense = nn.Dense(att_hops, activation=None, flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x): # F is the backend which implements the tensor operation\n",
    "        # x shape: [batch_size, seq_len, embedding_width]\n",
    "        # ut shape: [batch_size, seq_len, att_unit]\n",
    "        ut = self.ut_dense(x) # batch_size * seq_len [* embed_size * embed_size *] att_unit\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)# batch_size * seq_len [* att_unit * att_unit *] att_hops\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        # softmax is performed along the seq_len dimension\n",
    "        att = F.softmax(F.transpose(et, axes=(0, 2, 1)), axis=-1)\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = F.batch_dot(att, x)\n",
    "        # output is the weighted matrix representation of the matrix\n",
    "        # att is the weighted vector we use as attention\n",
    "        return output, att\n",
    "    \n",
    "# d_a = 20, hops = 5\n",
    "print(SelfAttention(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentiveBiLSTM(nn.HybridBlock):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, natt_unit, natt_hops, \\\n",
    "                 nfc, nclass, # these two params are not used currrently\n",
    "                 drop_prob, pool_way, prune_p=None, prune_q=None, **kwargs):\n",
    "        super(SelfAttentiveBiLSTM, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # now we switch back to shared layers\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            \n",
    "            self.bilstm = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, \\\n",
    "                                        bidirectional=True)\n",
    "            \n",
    "            self.att_encoder = SelfAttention(natt_unit, natt_hops)\n",
    "            self.dense = nn.Dense(nfc, activation='tanh')\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.Dense(nclass)\n",
    "            \n",
    "            self.dense_p, self.dense_q = None, None\n",
    "            if all([prune_p, prune_q]):\n",
    "                self.dense_p = nn.Dense(prune_p, activation='tanh', flatten=False)\n",
    "                self.dense_q = nn.Dense(prune_q, activation='tanh', flatten=False)\n",
    "\n",
    "            self.drop_prob = drop_prob\n",
    "            self.pool_way = pool_way\n",
    "\n",
    "    def hybrid_forward(self, F, inp):\n",
    "        # inp_embed size: [batch, seq_len, embed_size]\n",
    "        inp_embed = self.embedding_layer(inp)\n",
    "        # rnn requires the first dimension to be the time steps\n",
    "        h_output = self.bilstm(F.transpose(inp_embed, axes=(1, 0, 2)))\n",
    "        # att_output: [batch, att_hops, emsize]\n",
    "        output, att = self.att_encoder(F.transpose(h_output, axes=(1, 0, 2)))\n",
    "        '''\n",
    "        FIXME: now this code will only work with flatten\n",
    "        '''\n",
    "        dense_input = None\n",
    "        if self.pool_way == 'flatten':\n",
    "            dense_input = F.Dropout(F.flatten(output), self.drop_prob)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        '''\n",
    "        elif self.pool_way == 'mean':\n",
    "            dense_input = F.Dropout(F.mean(att_output, axis=1), self.drop_prob)\n",
    "        elif self.pool_way == 'prune' and all([self.dense_p, self.dense_q]):\n",
    "            # p_section: [batch, att_hops, prune_p]\n",
    "            p_section = self.dense_p(att_output)\n",
    "            # q_section: [batch, emsize, prune_q]\n",
    "            q_section = self.dense_q(F.transpose(att_output, axes=(0, 2, 1)))\n",
    "            dense_input = F.Dropout(F.concat(F.flatten(p_section), F.flatten(q_section), dim=-1), self.drop_prob)\n",
    "        '''\n",
    "        dense_out = self.dense(dense_input)\n",
    "        output = self.output_layer(F.Dropout(dense_out, self.drop_prob))\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize a new model\n",
      "SelfAttentiveBiLSTM(\n",
      "  (embedding_layer): Embedding(17473 -> 300, float32)\n",
      "  (bilstm): LSTM(None -> 400, TNC, num_layers=4, dropout=0.6, bidirectional)\n",
      "  (att_encoder): SelfAttention(\n",
      "    (ut_dense): Dense(None -> 400, Activation(tanh))\n",
      "    (et_dense): Dense(None -> 20, linear)\n",
      "  )\n",
      "  (dense): Dense(None -> 512, Activation(tanh))\n",
      "  (output_layer): Dense(None -> 72, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300   # word embedding size\n",
    "nhidden = 400    # lstm hidden_dim\n",
    "nlayers = 4     # lstm layers\n",
    "natt_unit = 400     # the hidden_units of attention layer\n",
    "natt_hops = 20    # the channels of attention\n",
    "nfc = 512  # last dense layer size\n",
    "nclass = 72 # we have 72 emoji in total\n",
    "\n",
    "drop_prob = 0.6\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "try:\n",
    "    assert(False)\n",
    "    model = gluon.nn.SymbolBlock.imports(\"model/model-symbol.json\", ['data'], \\\n",
    "                                         \"model/model-0001.params\", ctx=ctx)\n",
    "    print('use saved model params to start')\n",
    "except:\n",
    "    model = SelfAttentiveBiLSTM(vocab_len, emsize, nhidden, nlayers,\n",
    "                            natt_unit, natt_hops, nfc, nclass,\n",
    "                            drop_prob, pool_way, prune_p, prune_q)\n",
    "\n",
    "    print('initialize a new model')\n",
    "    model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "    model.hybridize()\n",
    "\n",
    "    # Attach a pre-trained glove word vector to the embedding layer\n",
    "    model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "    # fixed the embedding layer\n",
    "    model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-0.216152  0.111755  0.131167  0.108303 -0.56618   0.154915 -0.682917\n",
       "  0.184372 -1.259019 -0.270754 -0.295431 -0.477648  0.313637 -0.249563\n",
       "  0.464083 -0.177201  0.250866  0.63421  -0.023141 -0.083413 -0.086886\n",
       "  0.373867 -0.120577  0.154108  0.075599  0.749676  0.064633  0.346573\n",
       " -0.375857  0.170967 -0.387877  0.621435  0.252638 -0.327384  0.03449\n",
       "  0.59719   0.396766  0.550666 -0.658407 -0.525238  0.167532 -0.511287\n",
       " -0.360124 -0.815612 -0.511149 -0.866398  0.068793 -0.629899  0.036555\n",
       " -0.245903 -0.501821  0.222177 -0.887551 -0.059061  0.357666  0.444045\n",
       " -0.632446  0.706885  0.488229  0.459782  0.109316 -0.090775 -0.408769\n",
       "  0.25539   0.630114 -0.136657 -0.541437  0.510262 -0.273591  0.137092\n",
       " -0.586211 -0.199848  0.066356  0.603941 -0.288794 -0.023497 -0.258354\n",
       "  0.341849  0.427584 -0.001543  0.755926  0.718712 -1.017008  0.452808\n",
       "  0.173271  0.29188   0.644698  0.49299   0.216398  0.517657  0.00933\n",
       "  0.858918 -0.384057 -0.178975 -0.281533  1.395328  0.856061  0.560499\n",
       " -0.335769 -0.071375 -0.201207 -0.677237  0.587448  0.076399  1.100911\n",
       " -0.912248 -0.57333  -0.50441  -0.391133 -0.12867  -0.274657  0.448127\n",
       " -0.575652  0.33706  -0.149754  0.090467 -0.492194 -0.081431 -0.260503\n",
       "  0.111296  0.205207  0.260449  0.441846  0.679927  0.094967 -0.077246\n",
       " -0.478524  0.140242  0.270564 -0.381547 -0.240794  0.595419 -0.336331\n",
       " -0.991825  0.205368 -0.095051 -0.681293  0.986179  0.585376  0.474926\n",
       " -0.319181 -0.125708 -0.035296  1.298804  0.364361 -0.959611 -0.005816\n",
       " -0.257128  0.321672  0.24452  -0.057346  0.233022 -0.231019  0.689146\n",
       " -0.885067 -0.769348 -0.402978  0.006292 -0.008371  0.58447   0.214406\n",
       "  0.07519   0.26402   0.263128 -0.352657  0.107167 -0.754611  0.144382\n",
       " -0.223209 -0.001827 -0.413423  1.432451 -0.194973  0.434085  0.0993\n",
       "  0.546444 -0.587421  0.711087 -0.420685  0.111468 -0.449062  0.576244\n",
       "  0.176635 -0.728521 -0.182665 -0.338811  0.380492 -0.866309  0.022187\n",
       "  0.392927 -0.467832 -0.053406 -0.574791 -0.847851 -0.28072  -0.367377\n",
       " -0.210802  0.279111  0.67106   0.398311  0.401414  0.035663  0.361546\n",
       " -0.248559 -0.209737 -0.112492 -0.281607  0.365745  0.656992  0.79984\n",
       "  0.562788 -0.008432  0.088128 -0.539908 -1.034308 -0.213033  0.584708\n",
       "  0.097     0.192504  0.138019  0.021104 -0.273847 -0.396205  0.251443\n",
       " -0.233656  0.237069  0.284046 -0.522521 -0.304831 -0.2889    0.138421\n",
       "  0.671724 -0.382596  0.307982  0.282629  0.121924  0.37085   0.296891\n",
       " -0.321504  0.544077 -0.536023  0.238942  0.281211 -0.226763 -0.076582\n",
       "  0.009918 -0.582249  0.444073  0.252879 -0.341321  0.325932  0.583666\n",
       " -0.089971 -0.562199  0.217983  0.917098 -0.183431 -0.437333 -0.760043\n",
       "  0.129498  0.032326 -0.173446 -0.750573  0.273462  0.309773 -0.474446\n",
       "  0.173527 -0.082014  0.429216 -0.298323  0.136077 -0.428049  0.393772\n",
       "  0.23411  -0.313742  0.292258 -0.15159   0.098632 -0.177718 -0.301809\n",
       "  0.383317 -0.400927  0.021241 -0.390365  0.165357 -0.215971 -0.31047\n",
       " -0.099302 -0.629251  0.143659  0.10045  -0.031504  0.359497 -1.069457\n",
       "  0.024579  0.060824 -0.446689  0.821554  0.157456 -0.163165]\n",
       "<NDArray 300 @cpu(0)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding.idx_to_vec[vocab.embedding.token_to_idx['i']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helpers\n",
    "Calculate loss, one epoch computation and top function for train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, y, model, loss, class_weight, penal_coeff):\n",
    "    pred, att = model(x)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name in ['sce', 'l1', 'l2']:\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    # penalty, now we have two att's\n",
    "    diversity_penalty = nd.batch_dot(att, nd.transpose(att, axes=(0, 2, 1))) - \\\n",
    "                        nd.eye(att.shape[1], ctx=att.context)\n",
    "    l = l + penal_coeff * diversity_penalty.norm(axis=(1, 2))\n",
    "\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch,\n",
    "              penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x, batch_y in data_iter:\n",
    "        batch_x = batch_x.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                               loss, class_weight, penal_coeff)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                           loss, class_weight, penal_coeff)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 3 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs,\n",
    "                penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now we will train this model. To handle data inbalance, we first set an estimated weight of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0438551041853086, 2.1327178079706437, 2.6289386989030965, 1.3132616875182228, 5.865340939109901, 5.4786618184253655, 4.224614467097051, 5.801492131646614, 4.885536195487413, 4.542375563350704, 4.081807483487323, 5.189702164863072, 3.305837627261018, 4.878938941572088, 3.6444791357195254, 3.7205017060522403, 4.472592473586576, 4.093379015241236, 4.248802590468786, 3.0519071826380233, 5.122529615543214, 5.458262330069614, 4.908613508965677, 3.5998957691127362, 4.323533509839654, 5.63636542003083, 3.9929312237387604, 3.03816172414858, 3.838505771921529, 3.3860142787030894, 4.648330755306223, 4.366421424733982, 3.0369374921158596, 4.546694392249273, 4.857127296782221, 4.99100541119585, 4.843795600702158, 4.4413003436993606, 4.398435138826409, 4.204894865312225, 2.853687744438452, 5.250217403899569, 4.900510567088717, 4.997759625484381, 4.783218487698108, 5.317967644021719, 6.028575926328332, 4.697668606671044, 4.440563477981859, 6.461061349927445, 3.794757404347419, 6.040033882163419, 4.6115318185662595, 5.955852980886528, 5.710811484156139, 4.524892338235817, 4.8436547665193865, 5.634137530831431, 4.403465922039879, 5.049895445857338, 4.365826212453852, 6.2453129112369385, 5.361965367639121, 5.198410218719789, 5.917371769581856, 5.37969483511594, 5.655997094842368, 5.106856259425129, 5.765226931301615, 5.550102187103543, 5.973200668498348, 6.430667419985345]\n"
     ]
    }
   ],
   "source": [
    "from util import get_weight\n",
    "weight_list = get_weight(DATA_FOLDER, LABEL_FILE)\n",
    "print(weight_list)\n",
    "class_weight = None\n",
    "loss_name = 'wsce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "penal_coeff = 0.003\n",
    "clip = .5\n",
    "nepochs = 20\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array(weight_list, ctx=ctx)\n",
    "elif loss_name == 'l1':\n",
    "    loss = gluon.loss.L1Loss()\n",
    "elif loss_name == 'l2':\n",
    "    loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 16.6725, acc_train 0.063, F1_train 0.036, \n",
      "\t valid_loss 15.4185, acc_valid 0.081, F1_valid 0.016, \n",
      "time 9.86 sec\n",
      "****************************************************************************************************\n",
      "epoch 2, learning_rate 0.00100 \n",
      "\t train_loss 16.3687, acc_train 0.064, F1_train 0.034, \n",
      "\t valid_loss 15.7555, acc_valid 0.044, F1_valid 0.016, \n",
      "time 10.00 sec\n",
      "****************************************************************************************************\n",
      "epoch 3, learning_rate 0.00100 \n",
      "\t train_loss 16.0826, acc_train 0.073, F1_train 0.054, \n",
      "\t valid_loss 15.5848, acc_valid 0.105, F1_valid 0.041, \n",
      "time 10.02 sec\n",
      "****************************************************************************************************\n",
      "epoch 4, learning_rate 0.00090 \n",
      "\t train_loss 15.9632, acc_train 0.069, F1_train 0.041, \n",
      "\t valid_loss 15.5278, acc_valid 0.062, F1_valid 0.012, \n",
      "time 10.02 sec\n",
      "****************************************************************************************************\n",
      "epoch 5, learning_rate 0.00090 \n",
      "\t train_loss 16.1114, acc_train 0.079, F1_train 0.051, \n",
      "\t valid_loss 15.2852, acc_valid 0.061, F1_valid 0.008, \n",
      "time 10.07 sec\n",
      "****************************************************************************************************\n",
      "epoch 6, learning_rate 0.00090 \n",
      "\t train_loss 15.7119, acc_train 0.091, F1_train 0.057, \n",
      "\t valid_loss 15.3081, acc_valid 0.093, F1_valid 0.030, \n",
      "time 10.18 sec\n",
      "****************************************************************************************************\n",
      "epoch 7, learning_rate 0.00081 \n",
      "\t train_loss 15.7486, acc_train 0.082, F1_train 0.054, \n",
      "\t valid_loss 15.4231, acc_valid 0.083, F1_valid 0.014, \n",
      "time 10.09 sec\n",
      "****************************************************************************************************\n",
      "epoch 8, learning_rate 0.00081 \n",
      "\t train_loss 15.7972, acc_train 0.073, F1_train 0.050, \n",
      "\t valid_loss 15.2297, acc_valid 0.036, F1_valid 0.018, \n",
      "time 10.07 sec\n",
      "****************************************************************************************************\n",
      "epoch 9, learning_rate 0.00081 \n",
      "\t train_loss 15.4476, acc_train 0.087, F1_train 0.060, \n",
      "\t valid_loss 15.4562, acc_valid 0.028, F1_valid 0.016, \n",
      "time 10.07 sec\n",
      "****************************************************************************************************\n",
      "epoch 10, learning_rate 0.00073 \n",
      "\t train_loss 15.3460, acc_train 0.090, F1_train 0.061, \n",
      "\t valid_loss 15.1775, acc_valid 0.090, F1_valid 0.025, \n",
      "time 10.08 sec\n",
      "****************************************************************************************************\n",
      "epoch 11, learning_rate 0.00073 \n",
      "\t train_loss 15.5588, acc_train 0.083, F1_train 0.052, \n",
      "\t valid_loss 15.3048, acc_valid 0.091, F1_valid 0.026, \n",
      "time 10.12 sec\n",
      "****************************************************************************************************\n",
      "epoch 12, learning_rate 0.00073 \n",
      "\t train_loss 15.3693, acc_train 0.089, F1_train 0.063, \n",
      "\t valid_loss 15.1300, acc_valid 0.090, F1_valid 0.028, \n",
      "time 10.12 sec\n",
      "****************************************************************************************************\n",
      "epoch 13, learning_rate 0.00066 \n",
      "\t train_loss 15.2518, acc_train 0.092, F1_train 0.061, \n",
      "\t valid_loss 14.9853, acc_valid 0.090, F1_valid 0.030, \n",
      "time 10.20 sec\n",
      "****************************************************************************************************\n",
      "epoch 14, learning_rate 0.00066 \n",
      "\t train_loss 14.9257, acc_train 0.114, F1_train 0.077, \n",
      "\t valid_loss 15.4510, acc_valid 0.073, F1_valid 0.032, \n",
      "time 10.28 sec\n",
      "****************************************************************************************************\n",
      "epoch 15, learning_rate 0.00066 \n",
      "\t train_loss 15.4011, acc_train 0.097, F1_train 0.065, \n",
      "\t valid_loss 15.2601, acc_valid 0.100, F1_valid 0.041, \n",
      "time 9.83 sec\n",
      "****************************************************************************************************\n",
      "epoch 16, learning_rate 0.00059 \n",
      "\t train_loss 14.8255, acc_train 0.109, F1_train 0.067, \n",
      "\t valid_loss 15.4677, acc_valid 0.103, F1_valid 0.041, \n",
      "time 9.97 sec\n",
      "****************************************************************************************************\n",
      "epoch 17, learning_rate 0.00059 \n",
      "\t train_loss 14.9704, acc_train 0.095, F1_train 0.065, \n",
      "\t valid_loss 15.0882, acc_valid 0.072, F1_valid 0.028, \n",
      "time 9.46 sec\n",
      "****************************************************************************************************\n",
      "epoch 18, learning_rate 0.00059 \n",
      "\t train_loss 14.7782, acc_train 0.110, F1_train 0.081, \n",
      "\t valid_loss 16.0485, acc_valid 0.049, F1_valid 0.032, \n",
      "time 9.50 sec\n",
      "****************************************************************************************************\n",
      "epoch 19, learning_rate 0.00053 \n",
      "\t train_loss 15.2609, acc_train 0.098, F1_train 0.072, \n",
      "\t valid_loss 15.0750, acc_valid 0.135, F1_valid 0.063, \n",
      "time 9.43 sec\n",
      "****************************************************************************************************\n",
      "epoch 20, learning_rate 0.00053 \n",
      "\t train_loss 14.5069, acc_train 0.119, F1_train 0.082, \n",
      "\t valid_loss 15.1070, acc_valid 0.109, F1_valid 0.050, \n",
      "time 9.65 sec\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, penal_coeff=penal_coeff, \\\n",
    "            clip=clip, class_weight=class_weight, loss_name=loss_name)\n",
    "token = str(round(time.time()))\n",
    "model.export(\"model/model\"+token, epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATA = 'test.csv'\n",
    "predictions = []\n",
    "test_df = pd.read_csv(DATA_FOLDER+TEST_DATA, header=None, sep='\\t')\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pred len 2000, time 30.26s\n",
      "current pred len 4000, time 30.39s\n",
      "current pred len 6000, time 30.38s\n",
      "current pred len 8000, time 30.28s\n",
      "current pred len 10000, time 30.28s\n",
      "current pred len 12000, time 31.28s\n",
      "current pred len 14000, time 31.20s\n",
      "current pred len 16000, time 30.63s\n",
      "current pred len 18000, time 30.59s\n",
      "current pred len 20000, time 31.37s\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "[10:45:56] src/operator/./cudnn_rnn-inl.h:710: Check failed: e == CUDNN_STATUS_SUCCESS (8 vs. 0) cuDNN: CUDNN_STATUS_EXECUTION_FAILED\n\nStack trace returned 10 entries:\n[bt] (0) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x40ba6a) [0x7fd464d15a6a]\n[bt] (1) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x40c081) [0x7fd464d16081]\n[bt] (2) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x5644cc3) [0x7fd469f4ecc3]\n[bt] (3) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x5646ccb) [0x7fd469f50ccb]\n[bt] (4) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2f88314) [0x7fd467892314]\n[bt] (5) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#3}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const+0x2f0) [0x7fd46767a7c0]\n[bt] (6) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#4}>::_M_invoke(std::_Any_data const&, mxnet::RunContext)+0x26) [0x7fd46767ae36]\n[bt] (7) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2cc15fd) [0x7fd4675cb5fd]\n[bt] (8) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2cc15e7) [0x7fd4675cb5e7]\n[bt] (9) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2cc15e7) [0x7fd4675cb5e7]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-bd77806c9e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1978\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1980\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1981\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \"\"\"\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: [10:45:56] src/operator/./cudnn_rnn-inl.h:710: Check failed: e == CUDNN_STATUS_SUCCESS (8 vs. 0) cuDNN: CUDNN_STATUS_EXECUTION_FAILED\n\nStack trace returned 10 entries:\n[bt] (0) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x40ba6a) [0x7fd464d15a6a]\n[bt] (1) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x40c081) [0x7fd464d16081]\n[bt] (2) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x5644cc3) [0x7fd469f4ecc3]\n[bt] (3) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x5646ccb) [0x7fd469f50ccb]\n[bt] (4) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2f88314) [0x7fd467892314]\n[bt] (5) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#3}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const+0x2f0) [0x7fd46767a7c0]\n[bt] (6) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#4}>::_M_invoke(std::_Any_data const&, mxnet::RunContext)+0x26) [0x7fd46767ae36]\n[bt] (7) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2cc15fd) [0x7fd4675cb5fd]\n[bt] (8) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2cc15e7) [0x7fd4675cb5e7]\n[bt] (9) /home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2cc15e7) [0x7fd4675cb5e7]\n\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for _, tweet in test_df.iterrows():\n",
    "    token = vocab[seg.cut(tweet[1])]\n",
    "    if token is None:\n",
    "        token = [0]\n",
    "    inp = nd.array(token, ctx=ctx).reshape(1,-1)\n",
    "    pred, _ = model(inp)\n",
    "    pred = nd.argmax(pred, axis=-1).asscalar()\n",
    "    predictions.append(int(pred))\n",
    "    if len(predictions)%2000==0:\n",
    "        ckpt = time.time()\n",
    "        print('current pred len %d, time %.2fs' % (len(predictions), ckpt-start))\n",
    "        start = ckpt\n",
    "submit = pd.DataFrame({'Expected': predictions})\n",
    "submit.to_csv('submission.csv', sep=',', index_label='ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
