{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "import gluonnlp as nlp\n",
    "import pkuseg\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from d2l import try_gpu\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TRAIN_DATA = 'train.csv'\n",
    "WORD_EMBED = 'sgns.weibo.bigram-char'\n",
    "LABEL_FILE = 'train.label'\n",
    "N_ROWS=10000\n",
    "ctx = try_gpu()\n",
    "seg = pkuseg.pkuseg(model_name='web')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_FOLDER+TRAIN_DATA, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776847, 86317)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =[ [row[0], row[1]] for _, row in train_df.iterrows()]\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, .1)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=86.11s, #Sentences=776847\n",
      "Done! Tokenizing Time=10.47s, #Sentences=86317\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(x):\n",
    "    tweet, label = x\n",
    "    if type(tweet) != str:\n",
    "        tweet = str(tweet)\n",
    "    word_list = seg.cut(tweet)\n",
    "    return word_list, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def to_word_list(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(tokenizer, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_word_list, train_word_lengths = to_word_list(train_dataset)\n",
    "valid_word_list, valid_word_lengths = to_word_list(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/embedding/token_embedding.py:296: UserWarning: line 0 in data/sgns.weibo.bigram-char: skipped likely header line.\n",
      "  .format(line_num, pretrained_file_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=100004, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "train_seqs = [sample[0] for sample in train_word_list]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=100000)\n",
    "\n",
    "# load customed pre-trained embedding\n",
    "embedding_weights = nlp.embedding.TokenEmbedding.from_file(file_path=DATA_FOLDER+WORD_EMBED)\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_word_list)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=776847, batch_num=4617\n",
      "  key=[22, 35, 48, 61, 74, 87, 100, 113, 126, 139]\n",
      "  cnt=[637662, 69980, 31150, 17042, 11028, 7082, 2748, 149, 3, 3]\n",
      "  batch_size=[202, 127, 92, 72, 64, 64, 64, 64, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # in this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_word_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [4.9000e+02 5.0000e+00 1.4615e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [1.5000e+01 9.2900e+02 5.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [6.3300e+02 5.1000e+01 3.2100e+02 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [3.2600e+02 1.6300e+03 1.1996e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [1.0500e+02 4.2000e+01 1.4000e+01 ... 0.0000e+00 0.0000e+00 0.0000e+00]]\n",
      "<NDArray 202x22 @cpu_shared(0)> \n",
      "[34 18 10 15  3 29 31 30 17  3 14  1 34 42 27 28 27 34 14 29 34 34 23 14\n",
      " 31 12 34 35 13 29 31 12 40 19 42 29 16  3 50 34 70  6 19 14 27 35 17 12\n",
      " 35 60 15 41 39 12 61 58 70 22 31 23  9 23 24  6 14 20 12  1 29 40 15 43\n",
      " 55 15 31  3  1  3 29 17  3 37 14 16 29 13 12 47 30  3  3 62 36 33 35 10\n",
      " 52 29 12  1 14 33  3  3  3 12 16 15 14 16 29 15 31 60  3 31 59  3 19 34\n",
      " 53 27 35 23 19 35 40 35 25 52 34 27 70 60 27  9 70 52 10 35 18 34 12 22\n",
      " 31 34 50  3 16  3 17 14 32 34 31 52 34  3 29 38  3  3  3 28 24 70 14 14\n",
      " 37 29 35 11 40 31 14 29 12 68 52 15  3 35 52 29 50 22 21 28 58 23 14 15\n",
      " 28 35 54 24 23 34 34 12 18  3]\n",
      "<NDArray 202 @cpu_shared(0)>\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in train_dataloader:\n",
    "    print(tweet, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model contruction\n",
    "Self attention layer, weighted cross entropy, and whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention(\n",
      "  (ut_dense): Dense(None -> 20, Activation(relu))\n",
      "  (et_dense): Dense(None -> 5, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# custom attention layer\n",
    "# in this class, we want to implement the operation:\n",
    "# softmax(W_2 * tanh(W_1 * H))\n",
    "# where H is the word embedding of the whole sentence, of shape (num_of_word, embed_size)\n",
    "class SelfAttention(nn.HybridBlock):\n",
    "    def __init__(self, att_unit, att_hops, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # this layer is tanh(w_1 * H), the att_unit corresponds to d_a in the essay\n",
    "            self.ut_dense = nn.Dense(att_unit, activation='relu', flatten=False)\n",
    "            # this layer implements the multiple hops\n",
    "            self.et_dense = nn.Dense(att_hops, activation=None, flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x): # F is the backend which implements the tensor operation\n",
    "        # x shape: [batch_size, seq_len, embedding_width]\n",
    "        # ut shape: [batch_size, seq_len, att_unit]\n",
    "        ut = self.ut_dense(x) # batch_size * seq_len [* embed_size * embed_size *] att_unit\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)# batch_size * seq_len [* att_unit * att_unit *] att_hops\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        # softmax is performed along the seq_len dimension\n",
    "        att = F.softmax(F.transpose(et, axes=(0, 2, 1)), axis=-1)\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = F.batch_dot(att, x)\n",
    "        # output is the weighted matrix representation of the matrix\n",
    "        # att is the weighted vector we use as attention\n",
    "        return output, att\n",
    "    \n",
    "# d_a = 20, hops = 5\n",
    "print(SelfAttention(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentiveBiLSTM(nn.HybridBlock):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, natt_unit, natt_hops, \\\n",
    "                 nfc, nclass, # these two params are not used currrently\n",
    "                 drop_prob, pool_way, prune_p=None, prune_q=None, **kwargs):\n",
    "        super(SelfAttentiveBiLSTM, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # now we switch back to shared layers\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            \n",
    "            self.bilstm = rnn.GRU(nhidden, num_layers=nlayers, dropout=drop_prob, \\\n",
    "                                        bidirectional=True)\n",
    "            \n",
    "            self.att_encoder = SelfAttention(natt_unit, natt_hops)\n",
    "            self.dense = nn.Dense(nfc, activation='relu')\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.Dense(nclass)\n",
    "            \n",
    "            self.dense_p, self.dense_q = None, None\n",
    "            if all([prune_p, prune_q]):\n",
    "                self.dense_p = nn.Dense(prune_p, activation='relu', flatten=False)\n",
    "                self.dense_q = nn.Dense(prune_q, activation='relu', flatten=False)\n",
    "\n",
    "            self.drop_prob = drop_prob\n",
    "            self.pool_way = pool_way\n",
    "\n",
    "    def hybrid_forward(self, F, inp):\n",
    "        # inp_embed size: [batch, seq_len, embed_size]\n",
    "        inp_embed = self.embedding_layer(inp)\n",
    "        # rnn requires the first dimension to be the time steps\n",
    "        h_output = self.bilstm(F.transpose(inp_embed, axes=(1, 0, 2)))\n",
    "        # att_output: [batch, att_hops, emsize]\n",
    "        output, att = self.att_encoder(F.transpose(h_output, axes=(1, 0, 2)))\n",
    "        '''\n",
    "        FIXME: now this code will only work with flatten\n",
    "        '''\n",
    "        dense_input = None\n",
    "        if self.pool_way == 'flatten':\n",
    "            dense_input = F.Dropout(F.flatten(output), self.drop_prob)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        '''\n",
    "        elif self.pool_way == 'mean':\n",
    "            dense_input = F.Dropout(F.mean(att_output, axis=1), self.drop_prob)\n",
    "        elif self.pool_way == 'prune' and all([self.dense_p, self.dense_q]):\n",
    "            # p_section: [batch, att_hops, prune_p]\n",
    "            p_section = self.dense_p(att_output)\n",
    "            # q_section: [batch, emsize, prune_q]\n",
    "            q_section = self.dense_q(F.transpose(att_output, axes=(0, 2, 1)))\n",
    "            dense_input = F.Dropout(F.concat(F.flatten(p_section), F.flatten(q_section), dim=-1), self.drop_prob)\n",
    "        '''\n",
    "        dense_out = self.dense(dense_input)\n",
    "        output = self.output_layer(F.Dropout(dense_out, self.drop_prob))\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize a new model\n",
      "SelfAttentiveBiLSTM(\n",
      "  (embedding_layer): Embedding(100004 -> 300, float32)\n",
      "  (bilstm): GRU(None -> 400, TNC, num_layers=4, dropout=0.3, bidirectional)\n",
      "  (att_encoder): SelfAttention(\n",
      "    (ut_dense): Dense(None -> 400, Activation(relu))\n",
      "    (et_dense): Dense(None -> 20, linear)\n",
      "  )\n",
      "  (dense): Dense(None -> 512, Activation(relu))\n",
      "  (output_layer): Dense(None -> 72, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300   # word embedding size\n",
    "nhidden = 400    # lstm hidden_dim\n",
    "nlayers = 4     # lstm layers\n",
    "natt_unit = 400     # the hidden_units of attention layer\n",
    "natt_hops = 20    # the channels of attention\n",
    "nfc = 512  # last dense layer size\n",
    "nclass = 72 # we have 72 emoji in total\n",
    "\n",
    "drop_prob = 0.3\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "try:\n",
    "    assert(False)\n",
    "    model = gluon.nn.SymbolBlock.imports(\"model/model-symbol.json\", ['data'], \\\n",
    "                                         \"model/model-0001.params\", ctx=ctx)\n",
    "    print('use saved model params to start')\n",
    "except:\n",
    "    model = SelfAttentiveBiLSTM(vocab_len, emsize, nhidden, nlayers,\n",
    "                            natt_unit, natt_hops, nfc, nclass,\n",
    "                            drop_prob, pool_way, prune_p, prune_q)\n",
    "\n",
    "    print('initialize a new model')\n",
    "    model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "    model.hybridize()\n",
    "\n",
    "    # Attach a pre-trained glove word vector to the embedding layer\n",
    "    model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "    # fixed the embedding layer\n",
    "    model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-0.216152  0.111755  0.131167  0.108303 -0.56618   0.154915 -0.682917\n",
       "  0.184372 -1.259019 -0.270754 -0.295431 -0.477648  0.313637 -0.249563\n",
       "  0.464083 -0.177201  0.250866  0.63421  -0.023141 -0.083413 -0.086886\n",
       "  0.373867 -0.120577  0.154108  0.075599  0.749676  0.064633  0.346573\n",
       " -0.375857  0.170967 -0.387877  0.621435  0.252638 -0.327384  0.03449\n",
       "  0.59719   0.396766  0.550666 -0.658407 -0.525238  0.167532 -0.511287\n",
       " -0.360124 -0.815612 -0.511149 -0.866398  0.068793 -0.629899  0.036555\n",
       " -0.245903 -0.501821  0.222177 -0.887551 -0.059061  0.357666  0.444045\n",
       " -0.632446  0.706885  0.488229  0.459782  0.109316 -0.090775 -0.408769\n",
       "  0.25539   0.630114 -0.136657 -0.541437  0.510262 -0.273591  0.137092\n",
       " -0.586211 -0.199848  0.066356  0.603941 -0.288794 -0.023497 -0.258354\n",
       "  0.341849  0.427584 -0.001543  0.755926  0.718712 -1.017008  0.452808\n",
       "  0.173271  0.29188   0.644698  0.49299   0.216398  0.517657  0.00933\n",
       "  0.858918 -0.384057 -0.178975 -0.281533  1.395328  0.856061  0.560499\n",
       " -0.335769 -0.071375 -0.201207 -0.677237  0.587448  0.076399  1.100911\n",
       " -0.912248 -0.57333  -0.50441  -0.391133 -0.12867  -0.274657  0.448127\n",
       " -0.575652  0.33706  -0.149754  0.090467 -0.492194 -0.081431 -0.260503\n",
       "  0.111296  0.205207  0.260449  0.441846  0.679927  0.094967 -0.077246\n",
       " -0.478524  0.140242  0.270564 -0.381547 -0.240794  0.595419 -0.336331\n",
       " -0.991825  0.205368 -0.095051 -0.681293  0.986179  0.585376  0.474926\n",
       " -0.319181 -0.125708 -0.035296  1.298804  0.364361 -0.959611 -0.005816\n",
       " -0.257128  0.321672  0.24452  -0.057346  0.233022 -0.231019  0.689146\n",
       " -0.885067 -0.769348 -0.402978  0.006292 -0.008371  0.58447   0.214406\n",
       "  0.07519   0.26402   0.263128 -0.352657  0.107167 -0.754611  0.144382\n",
       " -0.223209 -0.001827 -0.413423  1.432451 -0.194973  0.434085  0.0993\n",
       "  0.546444 -0.587421  0.711087 -0.420685  0.111468 -0.449062  0.576244\n",
       "  0.176635 -0.728521 -0.182665 -0.338811  0.380492 -0.866309  0.022187\n",
       "  0.392927 -0.467832 -0.053406 -0.574791 -0.847851 -0.28072  -0.367377\n",
       " -0.210802  0.279111  0.67106   0.398311  0.401414  0.035663  0.361546\n",
       " -0.248559 -0.209737 -0.112492 -0.281607  0.365745  0.656992  0.79984\n",
       "  0.562788 -0.008432  0.088128 -0.539908 -1.034308 -0.213033  0.584708\n",
       "  0.097     0.192504  0.138019  0.021104 -0.273847 -0.396205  0.251443\n",
       " -0.233656  0.237069  0.284046 -0.522521 -0.304831 -0.2889    0.138421\n",
       "  0.671724 -0.382596  0.307982  0.282629  0.121924  0.37085   0.296891\n",
       " -0.321504  0.544077 -0.536023  0.238942  0.281211 -0.226763 -0.076582\n",
       "  0.009918 -0.582249  0.444073  0.252879 -0.341321  0.325932  0.583666\n",
       " -0.089971 -0.562199  0.217983  0.917098 -0.183431 -0.437333 -0.760043\n",
       "  0.129498  0.032326 -0.173446 -0.750573  0.273462  0.309773 -0.474446\n",
       "  0.173527 -0.082014  0.429216 -0.298323  0.136077 -0.428049  0.393772\n",
       "  0.23411  -0.313742  0.292258 -0.15159   0.098632 -0.177718 -0.301809\n",
       "  0.383317 -0.400927  0.021241 -0.390365  0.165357 -0.215971 -0.31047\n",
       " -0.099302 -0.629251  0.143659  0.10045  -0.031504  0.359497 -1.069457\n",
       "  0.024579  0.060824 -0.446689  0.821554  0.157456 -0.163165]\n",
       "<NDArray 300 @cpu(0)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding.idx_to_vec[vocab.embedding.token_to_idx['i']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helpers\n",
    "Calculate loss, one epoch computation and top function for train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, y, model, loss, class_weight, penal_coeff):\n",
    "    pred, att = model(x)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name in ['sce', 'l1', 'l2']:\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    # penalty, now we have two att's\n",
    "    diversity_penalty = nd.batch_dot(att, nd.transpose(att, axes=(0, 2, 1))) - \\\n",
    "                        nd.eye(att.shape[1], ctx=att.context)\n",
    "    l = l + penal_coeff * diversity_penalty.norm(axis=(1, 2))\n",
    "\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch,\n",
    "              penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x, batch_y in data_iter:\n",
    "        batch_x = batch_x.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                               loss, class_weight, penal_coeff)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x, batch_y, model, \\\n",
    "                                           loss, class_weight, penal_coeff)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 3 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs,\n",
    "                penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now we will train this model. To handle data inbalance, we first set an estimated weight of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.71540487119179, 6.231636866323191, 8.231834498039241, 1.0, 18.57280561073304, 17.401745026992835, 13.552092608433227, 18.37972211720023, 15.594356705916198, 14.53901176939084, 13.104536718725747, 16.52330626689046, 10.602513556421325, 15.574148561247236, 11.712361031570136, 11.957176969460722, 14.323192522444323, 13.140910385186368, 13.627624950204199, 9.743139048861558, 16.3185802891442, 17.339836775281242, 15.665023296713656, 11.568132631262811, 13.860529807325692, 17.879875825858495, 12.824470864344555, 9.695774830353102, 12.334632996030269, 10.868524596249232, 14.865853919439653, 13.99389658605509, 9.691551652607913, 14.552353595700842, 15.507316853017446, 15.917054133678395, 15.466452578238123, 14.226260120483808, 14.093314603527972, 13.490458202189915, 9.049913740536173, 16.70756081057398, 15.640214579530252, 15.937696994349995, 15.28061912035655, 16.913653306770467, 19.06601879298311, 15.01772544677559, 14.223976363593618, 20.370409760379705, 12.195038400645434, 19.100617927084482, 14.752449675825199, 18.846357992542366, 18.105319867405903, 14.484984552767248, 15.466020830378127, 17.87312666665645, 14.108927512540324, 16.096952984557255, 13.992047099008975, 19.720080377748694, 17.047390144424135, 16.5498305478271, 18.730079557848377, 17.10125950474707, 17.939341623363983, 16.270779581361833, 18.270008398196094, 17.618439533944194, 18.89876694016796, 20.278832882550226]\n"
     ]
    }
   ],
   "source": [
    "from util import get_weight\n",
    "weight_list = get_weight(DATA_FOLDER, LABEL_FILE)\n",
    "print(weight_list)\n",
    "class_weight = None\n",
    "loss_name = 'sce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "penal_coeff = 0.0003\n",
    "clip = .5\n",
    "nepochs = 5\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array(weight_list, ctx=ctx)\n",
    "elif loss_name == 'l1':\n",
    "    loss = gluon.loss.L1Loss()\n",
    "elif loss_name == 'l2':\n",
    "    loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 400, batch_train_loss 3.3347, batch_train_acc 0.205\n",
      "epoch 1, batch 800, batch_train_loss 3.2992, batch_train_acc 0.220\n",
      "epoch 1, batch 1200, batch_train_loss 3.5468, batch_train_acc 0.134\n",
      "epoch 1, batch 1600, batch_train_loss 3.4170, batch_train_acc 0.158\n",
      "epoch 1, batch 2000, batch_train_loss 2.9928, batch_train_acc 0.264\n",
      "epoch 1, batch 2400, batch_train_loss 3.4491, batch_train_acc 0.153\n",
      "epoch 1, batch 2800, batch_train_loss 3.3380, batch_train_acc 0.163\n",
      "epoch 1, batch 3200, batch_train_loss 3.3341, batch_train_acc 0.203\n",
      "epoch 1, batch 3600, batch_train_loss 3.3482, batch_train_acc 0.183\n",
      "epoch 1, batch 4000, batch_train_loss 3.3876, batch_train_acc 0.183\n",
      "epoch 1, batch 4400, batch_train_loss 3.0698, batch_train_acc 0.181\n",
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 3.3303, acc_train 0.181, F1_train 0.119, \n",
      "\t valid_loss 3.3811, acc_valid 0.176, F1_valid 0.109, \n",
      "time 799.34 sec\n",
      "****************************************************************************************************\n",
      "epoch 2, batch 400, batch_train_loss 3.2792, batch_train_acc 0.178\n",
      "epoch 2, batch 800, batch_train_loss 3.2262, batch_train_acc 0.193\n",
      "epoch 2, batch 1200, batch_train_loss 3.3646, batch_train_acc 0.208\n",
      "epoch 2, batch 1600, batch_train_loss 3.1297, batch_train_acc 0.220\n",
      "epoch 2, batch 2000, batch_train_loss 3.5226, batch_train_acc 0.153\n",
      "epoch 2, batch 2400, batch_train_loss 3.3719, batch_train_acc 0.178\n",
      "epoch 2, batch 2800, batch_train_loss 3.2896, batch_train_acc 0.203\n",
      "epoch 2, batch 3200, batch_train_loss 3.3437, batch_train_acc 0.203\n",
      "epoch 2, batch 3600, batch_train_loss 3.2825, batch_train_acc 0.163\n",
      "epoch 2, batch 4000, batch_train_loss 3.3085, batch_train_acc 0.168\n",
      "epoch 2, batch 4400, batch_train_loss 3.2679, batch_train_acc 0.213\n",
      "epoch 2, learning_rate 0.00100 \n",
      "\t train_loss 3.2980, acc_train 0.185, F1_train 0.123, \n",
      "\t valid_loss 3.5631, acc_valid 0.150, F1_valid 0.092, \n",
      "time 773.01 sec\n",
      "****************************************************************************************************\n",
      "epoch 3, batch 400, batch_train_loss 3.4321, batch_train_acc 0.158\n",
      "epoch 3, batch 800, batch_train_loss 3.5744, batch_train_acc 0.168\n",
      "epoch 3, batch 1200, batch_train_loss 3.4359, batch_train_acc 0.168\n",
      "epoch 3, batch 1600, batch_train_loss 3.3342, batch_train_acc 0.173\n",
      "epoch 3, batch 2000, batch_train_loss 3.4555, batch_train_acc 0.158\n",
      "epoch 3, batch 2400, batch_train_loss 3.2287, batch_train_acc 0.219\n",
      "epoch 3, batch 2800, batch_train_loss 3.3711, batch_train_acc 0.139\n",
      "epoch 3, batch 3200, batch_train_loss 3.2362, batch_train_acc 0.218\n",
      "epoch 3, batch 3600, batch_train_loss 2.8441, batch_train_acc 0.344\n",
      "epoch 3, batch 4000, batch_train_loss 3.3244, batch_train_acc 0.183\n",
      "epoch 3, batch 4400, batch_train_loss 2.9847, batch_train_acc 0.250\n",
      "epoch 3, learning_rate 0.00100 \n",
      "\t train_loss 3.2625, acc_train 0.189, F1_train 0.128, \n",
      "\t valid_loss 3.3986, acc_valid 0.174, F1_valid 0.110, \n",
      "time 773.85 sec\n",
      "****************************************************************************************************\n",
      "epoch 4, batch 400, batch_train_loss 2.7819, batch_train_acc 0.283\n",
      "epoch 4, batch 800, batch_train_loss 3.3019, batch_train_acc 0.243\n",
      "epoch 4, batch 1200, batch_train_loss 3.2702, batch_train_acc 0.208\n",
      "epoch 4, batch 1600, batch_train_loss 2.9766, batch_train_acc 0.234\n",
      "epoch 4, batch 2000, batch_train_loss 3.4117, batch_train_acc 0.163\n",
      "epoch 4, batch 2400, batch_train_loss 3.3223, batch_train_acc 0.193\n",
      "epoch 4, batch 2800, batch_train_loss 3.3507, batch_train_acc 0.173\n",
      "epoch 4, batch 3200, batch_train_loss 3.3689, batch_train_acc 0.173\n",
      "epoch 4, batch 3600, batch_train_loss 3.2690, batch_train_acc 0.153\n",
      "epoch 4, batch 4000, batch_train_loss 3.3733, batch_train_acc 0.158\n",
      "epoch 4, batch 4400, batch_train_loss 3.1586, batch_train_acc 0.250\n",
      "epoch 4, learning_rate 0.00090 \n",
      "\t train_loss 3.2253, acc_train 0.194, F1_train 0.133, \n",
      "\t valid_loss 3.3709, acc_valid 0.177, F1_valid 0.118, \n",
      "time 808.24 sec\n",
      "****************************************************************************************************\n",
      "epoch 5, batch 400, batch_train_loss 2.8295, batch_train_acc 0.261\n",
      "epoch 5, batch 800, batch_train_loss 2.6100, batch_train_acc 0.348\n",
      "epoch 5, batch 1200, batch_train_loss 2.9684, batch_train_acc 0.281\n",
      "epoch 5, batch 1600, batch_train_loss 3.0893, batch_train_acc 0.276\n",
      "epoch 5, batch 2000, batch_train_loss 3.2619, batch_train_acc 0.158\n",
      "epoch 5, batch 2400, batch_train_loss 3.2787, batch_train_acc 0.198\n",
      "epoch 5, batch 2800, batch_train_loss 3.1623, batch_train_acc 0.203\n",
      "epoch 5, batch 3200, batch_train_loss 3.2384, batch_train_acc 0.218\n",
      "epoch 5, batch 3600, batch_train_loss 3.3040, batch_train_acc 0.208\n",
      "epoch 5, batch 4000, batch_train_loss 3.2744, batch_train_acc 0.149\n",
      "epoch 5, batch 4400, batch_train_loss 2.9250, batch_train_acc 0.348\n",
      "epoch 5, learning_rate 0.00090 \n",
      "\t train_loss 3.1911, acc_train 0.198, F1_train 0.138, \n",
      "\t valid_loss 3.3962, acc_valid 0.172, F1_valid 0.110, \n",
      "time 785.83 sec\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, penal_coeff=penal_coeff, \\\n",
    "            clip=clip, class_weight=class_weight, loss_name=loss_name)\n",
    "token = str(round(time.time()))\n",
    "model.export(\"model/self-att\"+token+'.params', epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATA = 'test.csv'\n",
    "predictions = []\n",
    "test_df = pd.read_csv(DATA_FOLDER+TEST_DATA, header=None, sep='\\t')\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pred len 54000, time 7.09s\n",
      "current pred len 56000, time 21.73s\n",
      "current pred len 58000, time 21.79s\n",
      "current pred len 60000, time 21.72s\n",
      "current pred len 62000, time 21.78s\n",
      "current pred len 64000, time 21.74s\n",
      "current pred len 66000, time 21.67s\n",
      "current pred len 68000, time 21.65s\n",
      "current pred len 70000, time 21.63s\n",
      "current pred len 72000, time 21.66s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-a4eac9ed7b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1978\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1980\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1981\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for _, tweet in test_df.iterrows():\n",
    "    token = vocab[seg.cut(tweet[1])]\n",
    "    if type(token) != str:\n",
    "        token = [0]\n",
    "    inp = nd.array(token, ctx=ctx).reshape(1,-1)\n",
    "    pred, _ = model(inp)\n",
    "    pred = nd.argmax(pred, axis=-1).asscalar()\n",
    "    predictions.append(int(pred))\n",
    "    if len(predictions)%2000==0:\n",
    "        ckpt = time.time()\n",
    "        print('current pred len %d, time %.2fs' % (len(predictions), ckpt-start))\n",
    "        start = ckpt\n",
    "submit = pd.DataFrame({'Expected': predictions})\n",
    "submit.to_csv('submission.csv', sep=',', index_label='ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
